{"cells":[{"cell_type":"markdown","metadata":{"id":"sgclVa4OK68R"},"source":["# import"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2233,"status":"ok","timestamp":1701861514364,"user":{"displayName":"盛拓矢","userId":"00009370238873879878"},"user_tz":-540},"id":"congtmOqK-M9","outputId":"47e106e9-10d6-47b7-fbb5-01ccc4f87cea"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","import os\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["# root_dir = \"/content/drive/MyDrive/Colab Notebooks/2023年度スポコン/データ/試合終了/video\"\n","\n","# video_names = os.listdir(root_dir)\n","\n","# # imgs = sorted(os.listdir(os.path.join(root_dir, video_names[0], \"images\")))"],"metadata":{"id":"7OmitWxRPvWb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# import sys\n","\n","# def do_system(arg):\n","# \tprint(f\"==== running: {arg}\")\n","# \terr = os.system(arg)\n","# \tif err:\n","# \t\tprint(\"FATAL: command failed\")\n","# \t\tsys.exit(err)\n","\n","# for i in video_names:\n","#   video_path = os.path.join(root_dir, i, \"images\")\n","#   os.system(f\"yes | ffmpeg -framerate 20 -i '{video_path}/%04d.png' -pix_fmt yuv420p '/content/drive/MyDrive/Colab Notebooks/2023年度スポコン/データ/mmaction_exe/video/{i}_1.mp4'\")"],"metadata":{"id":"GsFjvfnZPezV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5iun4TOrLAwn"},"source":["# ライブラリのダウンロード"]},{"cell_type":"markdown","metadata":{"id":"hGpDrsqYK12a"},"source":["lang segment anything"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":139436,"status":"ok","timestamp":1701851835524,"user":{"displayName":"盛拓矢","userId":"00009370238873879878"},"user_tz":-540},"id":"Mlo8txyWKwlx","outputId":"8fe2c028-084e-4019-d0f1-326695cebbc5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting git+https://github.com/luca-medeiros/lang-segment-anything.git\n","  Cloning https://github.com/luca-medeiros/lang-segment-anything.git to /tmp/pip-req-build-h1j54u9t\n","  Running command git clone --filter=blob:none --quiet https://github.com/luca-medeiros/lang-segment-anything.git /tmp/pip-req-build-h1j54u9t\n","  Resolved https://github.com/luca-medeiros/lang-segment-anything.git to commit 134a48a81ebd691686187ecce2d016fb327a8852\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Collecting groundingdino@ git+https://github.com/IDEA-Research/GroundingDINO.git (from lang-sam==0.1.0)\n","  Cloning https://github.com/IDEA-Research/GroundingDINO.git to /tmp/pip-install-j7q0rc3u/groundingdino_9a3e3af0d6cd48acb4fb4f9006bec5fd\n","  Running command git clone --filter=blob:none --quiet https://github.com/IDEA-Research/GroundingDINO.git /tmp/pip-install-j7q0rc3u/groundingdino_9a3e3af0d6cd48acb4fb4f9006bec5fd\n","  Resolved https://github.com/IDEA-Research/GroundingDINO.git to commit 16e6b4bfcf6ed3a6f0ef2bc5752d716657a6b9fe\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting segment-anything@ git+https://github.com/facebookresearch/segment-anything.git (from lang-sam==0.1.0)\n","  Cloning https://github.com/facebookresearch/segment-anything.git to /tmp/pip-install-j7q0rc3u/segment-anything_a8c7e1d50bd747e39aa887afc50b2624\n","  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/segment-anything.git /tmp/pip-install-j7q0rc3u/segment-anything_a8c7e1d50bd747e39aa887afc50b2624\n","  Resolved https://github.com/facebookresearch/segment-anything.git to commit 6fdee8f2727f4506cfbbe553e23b895e27956588\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting Pillow==9.3.0 (from lang-sam==0.1.0)\n","  Downloading Pillow-9.3.0-cp310-cp310-manylinux_2_28_x86_64.whl (3.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting gradio<4.0.0,>=3.24.1 (from lang-sam==0.1.0)\n","  Downloading gradio-3.50.2-py3-none-any.whl (20.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.3/20.3 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting huggingface-hub<0.17.0,>=0.16.4 (from lang-sam==0.1.0)\n","  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting lightning<3.0.0,>=2.0.1 (from lang-sam==0.1.0)\n","  Downloading lightning-2.1.2-py3-none-any.whl (2.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting numpy<2.0.0,>=1.24.2 (from lang-sam==0.1.0)\n","  Downloading numpy-1.26.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: opencv_python<5.0.0.0,>=4.7.0.72 in /usr/local/lib/python3.10/dist-packages (from lang-sam==0.1.0) (4.8.0.76)\n","Requirement already satisfied: transformers<5.0.0,>=4.27.4 in /usr/local/lib/python3.10/dist-packages (from lang-sam==0.1.0) (4.35.2)\n","Collecting aiofiles<24.0,>=22.0 (from gradio<4.0.0,>=3.24.1->lang-sam==0.1.0)\n","  Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n","Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio<4.0.0,>=3.24.1->lang-sam==0.1.0) (4.2.2)\n","Collecting fastapi (from gradio<4.0.0,>=3.24.1->lang-sam==0.1.0)\n","  Downloading fastapi-0.104.1-py3-none-any.whl (92 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.9/92.9 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting ffmpy (from gradio<4.0.0,>=3.24.1->lang-sam==0.1.0)\n","  Downloading ffmpy-0.3.1.tar.gz (5.5 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting gradio-client==0.6.1 (from gradio<4.0.0,>=3.24.1->lang-sam==0.1.0)\n","  Downloading gradio_client-0.6.1-py3-none-any.whl (299 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m299.2/299.2 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting httpx (from gradio<4.0.0,>=3.24.1->lang-sam==0.1.0)\n","  Downloading httpx-0.25.2-py3-none-any.whl (74 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio<4.0.0,>=3.24.1->lang-sam==0.1.0) (6.1.1)\n","Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio<4.0.0,>=3.24.1->lang-sam==0.1.0) (3.1.2)\n","Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio<4.0.0,>=3.24.1->lang-sam==0.1.0) (2.1.3)\n","Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio<4.0.0,>=3.24.1->lang-sam==0.1.0) (3.7.1)\n","Collecting orjson~=3.0 (from gradio<4.0.0,>=3.24.1->lang-sam==0.1.0)\n","  Downloading orjson-3.9.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.7/138.7 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio<4.0.0,>=3.24.1->lang-sam==0.1.0) (23.2)\n","Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio<4.0.0,>=3.24.1->lang-sam==0.1.0) (1.5.3)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from gradio<4.0.0,>=3.24.1->lang-sam==0.1.0) (1.10.13)\n","Collecting pydub (from gradio<4.0.0,>=3.24.1->lang-sam==0.1.0)\n","  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n","Collecting python-multipart (from gradio<4.0.0,>=3.24.1->lang-sam==0.1.0)\n","  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio<4.0.0,>=3.24.1->lang-sam==0.1.0) (6.0.1)\n","Requirement already satisfied: requests~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio<4.0.0,>=3.24.1->lang-sam==0.1.0) (2.31.0)\n","Collecting semantic-version~=2.0 (from gradio<4.0.0,>=3.24.1->lang-sam==0.1.0)\n","  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n","Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio<4.0.0,>=3.24.1->lang-sam==0.1.0) (4.5.0)\n","Collecting uvicorn>=0.14.0 (from gradio<4.0.0,>=3.24.1->lang-sam==0.1.0)\n","  Downloading uvicorn-0.24.0.post1-py3-none-any.whl (59 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.7/59.7 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting websockets<12.0,>=10.0 (from gradio<4.0.0,>=3.24.1->lang-sam==0.1.0)\n","  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==0.6.1->gradio<4.0.0,>=3.24.1->lang-sam==0.1.0) (2023.6.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<0.17.0,>=0.16.4->lang-sam==0.1.0) (3.13.1)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<0.17.0,>=0.16.4->lang-sam==0.1.0) (4.66.1)\n","Collecting lightning-utilities<2.0,>=0.8.0 (from lightning<3.0.0,>=2.0.1->lang-sam==0.1.0)\n","  Downloading lightning_utilities-0.10.0-py3-none-any.whl (24 kB)\n","Requirement already satisfied: torch<4.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from lightning<3.0.0,>=2.0.1->lang-sam==0.1.0) (2.1.0+cu118)\n","Collecting torchmetrics<3.0,>=0.7.0 (from lightning<3.0.0,>=2.0.1->lang-sam==0.1.0)\n","  Downloading torchmetrics-1.2.1-py3-none-any.whl (806 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m806.1/806.1 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pytorch-lightning (from lightning<3.0.0,>=2.0.1->lang-sam==0.1.0)\n","  Downloading pytorch_lightning-2.1.2-py3-none-any.whl (776 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m776.9/776.9 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.27.4->lang-sam==0.1.0) (2023.6.3)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.27.4->lang-sam==0.1.0) (0.15.0)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.27.4->lang-sam==0.1.0) (0.4.1)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from groundingdino@ git+https://github.com/IDEA-Research/GroundingDINO.git->lang-sam==0.1.0) (0.16.0+cu118)\n","Collecting addict (from groundingdino@ git+https://github.com/IDEA-Research/GroundingDINO.git->lang-sam==0.1.0)\n","  Downloading addict-2.4.0-py3-none-any.whl (3.8 kB)\n","Collecting yapf (from groundingdino@ git+https://github.com/IDEA-Research/GroundingDINO.git->lang-sam==0.1.0)\n","  Downloading yapf-0.40.2-py3-none-any.whl (254 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.7/254.7 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting timm (from groundingdino@ git+https://github.com/IDEA-Research/GroundingDINO.git->lang-sam==0.1.0)\n","  Downloading timm-0.9.12-py3-none-any.whl (2.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting supervision==0.6.0 (from groundingdino@ git+https://github.com/IDEA-Research/GroundingDINO.git->lang-sam==0.1.0)\n","  Downloading supervision-0.6.0-py3-none-any.whl (31 kB)\n","Requirement already satisfied: pycocotools in /usr/local/lib/python3.10/dist-packages (from groundingdino@ git+https://github.com/IDEA-Research/GroundingDINO.git->lang-sam==0.1.0) (2.0.7)\n","Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio<4.0.0,>=3.24.1->lang-sam==0.1.0) (0.4)\n","Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio<4.0.0,>=3.24.1->lang-sam==0.1.0) (4.19.2)\n","Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio<4.0.0,>=3.24.1->lang-sam==0.1.0) (0.12.0)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec->gradio-client==0.6.1->gradio<4.0.0,>=3.24.1->lang-sam==0.1.0) (3.9.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities<2.0,>=0.8.0->lightning<3.0.0,>=2.0.1->lang-sam==0.1.0) (67.7.2)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio<4.0.0,>=3.24.1->lang-sam==0.1.0) (1.2.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio<4.0.0,>=3.24.1->lang-sam==0.1.0) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio<4.0.0,>=3.24.1->lang-sam==0.1.0) (4.45.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio<4.0.0,>=3.24.1->lang-sam==0.1.0) (1.4.5)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio<4.0.0,>=3.24.1->lang-sam==0.1.0) (3.1.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio<4.0.0,>=3.24.1->lang-sam==0.1.0) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio<4.0.0,>=3.24.1->lang-sam==0.1.0) (2023.3.post1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio<4.0.0,>=3.24.1->lang-sam==0.1.0) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio<4.0.0,>=3.24.1->lang-sam==0.1.0) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio<4.0.0,>=3.24.1->lang-sam==0.1.0) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio<4.0.0,>=3.24.1->lang-sam==0.1.0) (2023.11.17)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.12.0->lightning<3.0.0,>=2.0.1->lang-sam==0.1.0) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.12.0->lightning<3.0.0,>=2.0.1->lang-sam==0.1.0) (3.2.1)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.12.0->lightning<3.0.0,>=2.0.1->lang-sam==0.1.0) (2.1.0)\n","Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio<4.0.0,>=3.24.1->lang-sam==0.1.0) (8.1.7)\n","Collecting h11>=0.8 (from uvicorn>=0.14.0->gradio<4.0.0,>=3.24.1->lang-sam==0.1.0)\n","  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: anyio<4.0.0,>=3.7.1 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio<4.0.0,>=3.24.1->lang-sam==0.1.0) (3.7.1)\n","Collecting starlette<0.28.0,>=0.27.0 (from fastapi->gradio<4.0.0,>=3.24.1->lang-sam==0.1.0)\n","  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting typing-extensions~=4.0 (from gradio<4.0.0,>=3.24.1->lang-sam==0.1.0)\n","  Downloading typing_extensions-4.8.0-py3-none-any.whl (31 kB)\n","Collecting httpcore==1.* (from httpx->gradio<4.0.0,>=3.24.1->lang-sam==0.1.0)\n","  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio<4.0.0,>=3.24.1->lang-sam==0.1.0) (1.3.0)\n","Requirement already satisfied: importlib-metadata>=6.6.0 in /usr/local/lib/python3.10/dist-packages (from yapf->groundingdino@ git+https://github.com/IDEA-Research/GroundingDINO.git->lang-sam==0.1.0) (6.8.0)\n","Requirement already satisfied: platformdirs>=3.5.1 in /usr/local/lib/python3.10/dist-packages (from yapf->groundingdino@ git+https://github.com/IDEA-Research/GroundingDINO.git->lang-sam==0.1.0) (4.0.0)\n","Requirement already satisfied: tomli>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from yapf->groundingdino@ git+https://github.com/IDEA-Research/GroundingDINO.git->lang-sam==0.1.0) (2.0.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->gradio-client==0.6.1->gradio<4.0.0,>=3.24.1->lang-sam==0.1.0) (23.1.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->gradio-client==0.6.1->gradio<4.0.0,>=3.24.1->lang-sam==0.1.0) (6.0.4)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->gradio-client==0.6.1->gradio<4.0.0,>=3.24.1->lang-sam==0.1.0) (1.9.3)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->gradio-client==0.6.1->gradio<4.0.0,>=3.24.1->lang-sam==0.1.0) (1.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->gradio-client==0.6.1->gradio<4.0.0,>=3.24.1->lang-sam==0.1.0) (1.3.1)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->gradio-client==0.6.1->gradio<4.0.0,>=3.24.1->lang-sam==0.1.0) (4.0.3)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi->gradio<4.0.0,>=3.24.1->lang-sam==0.1.0) (1.2.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=6.6.0->yapf->groundingdino@ git+https://github.com/IDEA-Research/GroundingDINO.git->lang-sam==0.1.0) (3.17.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio<4.0.0,>=3.24.1->lang-sam==0.1.0) (2023.11.2)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio<4.0.0,>=3.24.1->lang-sam==0.1.0) (0.31.1)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio<4.0.0,>=3.24.1->lang-sam==0.1.0) (0.13.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio<4.0.0,>=3.24.1->lang-sam==0.1.0) (1.16.0)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<4.0,>=1.12.0->lightning<3.0.0,>=2.0.1->lang-sam==0.1.0) (1.3.0)\n","Building wheels for collected packages: lang-sam, groundingdino, segment-anything, ffmpy\n","  Building wheel for lang-sam (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for lang-sam: filename=lang_sam-0.1.0-py3-none-any.whl size=9714 sha256=747acbaa957d89ed4ca627797b9207c0db04db9a202ce85f1858ee9cd798e706\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-irpt97rs/wheels/c7/1d/cd/6f34f8ca4fbc01b7a1f522cf9c4654cbe4fcd4268069d6e739\n","  Building wheel for groundingdino (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for groundingdino: filename=groundingdino-0.1.0-cp310-cp310-linux_x86_64.whl size=2881822 sha256=526d6bd80763c7f8034352c24dc22a02d4e1155b9606d53b28b21730808ccf49\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-irpt97rs/wheels/6b/06/d7/b57f601a4df56af41d262a5b1b496359b13c323bf5ef0434b2\n","  Building wheel for segment-anything (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for segment-anything: filename=segment_anything-1.0-py3-none-any.whl size=36587 sha256=9738c3dfcdd66d344285a6554c5fa6ffd87ba3b61b48ba8ce527b323fe9c083e\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-irpt97rs/wheels/10/cf/59/9ccb2f0a1bcc81d4fbd0e501680b5d088d690c6cfbc02dc99d\n","  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for ffmpy: filename=ffmpy-0.3.1-py3-none-any.whl size=5579 sha256=358b3fe7eb170a41bc4bc05c72cf3dc9a025b47f47206b242e9133fa674ffa11\n","  Stored in directory: /root/.cache/pip/wheels/01/a6/d1/1c0828c304a4283b2c1639a09ad86f83d7c487ef34c6b4a1bf\n","Successfully built lang-sam groundingdino segment-anything ffmpy\n","Installing collected packages: segment-anything, pydub, ffmpy, addict, websockets, typing-extensions, semantic-version, python-multipart, Pillow, orjson, numpy, h11, aiofiles, yapf, uvicorn, starlette, lightning-utilities, huggingface-hub, httpcore, torchmetrics, httpx, fastapi, timm, supervision, pytorch-lightning, gradio-client, lightning, groundingdino, gradio, lang-sam\n","  Attempting uninstall: typing-extensions\n","    Found existing installation: typing_extensions 4.5.0\n","    Uninstalling typing_extensions-4.5.0:\n","      Successfully uninstalled typing_extensions-4.5.0\n","  Attempting uninstall: Pillow\n","    Found existing installation: Pillow 9.4.0\n","    Uninstalling Pillow-9.4.0:\n","      Successfully uninstalled Pillow-9.4.0\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.23.5\n","    Uninstalling numpy-1.23.5:\n","      Successfully uninstalled numpy-1.23.5\n","  Attempting uninstall: huggingface-hub\n","    Found existing installation: huggingface-hub 0.19.4\n","    Uninstalling huggingface-hub-0.19.4:\n","      Successfully uninstalled huggingface-hub-0.19.4\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","lida 0.0.10 requires kaleido, which is not installed.\n","cupy-cuda11x 11.0.0 requires numpy<1.26,>=1.20, but you have numpy 1.26.2 which is incompatible.\n","tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.8.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed Pillow-9.3.0 addict-2.4.0 aiofiles-23.2.1 fastapi-0.104.1 ffmpy-0.3.1 gradio-3.50.2 gradio-client-0.6.1 groundingdino-0.1.0 h11-0.14.0 httpcore-1.0.2 httpx-0.25.2 huggingface-hub-0.16.4 lang-sam-0.1.0 lightning-2.1.2 lightning-utilities-0.10.0 numpy-1.26.2 orjson-3.9.10 pydub-0.25.1 python-multipart-0.0.6 pytorch-lightning-2.1.2 segment-anything-1.0 semantic-version-2.10.0 starlette-0.27.0 supervision-0.6.0 timm-0.9.12 torchmetrics-1.2.1 typing-extensions-4.8.0 uvicorn-0.24.0.post1 websockets-11.0.3 yapf-0.40.2\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["PIL"]}}},"metadata":{}}],"source":["# import\n","import sys\n","import os\n","import distutils.core\n","\n","# import some common libraries\n","import numpy as np\n","import json\n","import cv2\n","import random\n","\n","\n","# import some common detectron2 utilities\n","# from detectron2 import model_zoo\n","# from detectron2.engine import DefaultPredictor\n","# from detectron2.config import get_cfg\n","# from detectron2.utils.visualizer import Visualizer\n","# from detectron2.data import MetadataCatalog, DatasetCatalog\n","\n","# # 画像から動画変換用\n","# import matplotlib.pyplot as plt\n","# import subprocess\n","\n","# Lang-segment用\n","!pip install -U git+https://github.com/luca-medeiros/lang-segment-anything.git\n","import warnings\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import requests\n","from PIL import Image\n","from io import BytesIO\n","from lang_sam import LangSAM"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gRSvvAVihR0v"},"outputs":[],"source":["def all_video_lang_segment_anything(inpaint_path, output_path):\n","  # Suppress warning messages\n","  warnings.filterwarnings(\"ignore\")\n","  text_prompt = \"two person in white and in blue on the yellow floor, the yellow floor.\"\n","  try:\n","      # if image.startswith(\"http\"):\n","      #     image_pil = download_image(image)\n","      # else:\n","      #     image_pil = Image.open(image).convert(\"RGB\")\n","\n","      model = LangSAM()\n","\n","      start_frame = 0\n","      fps_ls = []\n","      img_path_ls = []\n","\n","      count = 0\n","      print(inpaint_path)\n","      cap = cv2.VideoCapture(inpaint_path)\n","\n","      cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n","\n","      fps_ls.append(int(cap.get(cv2.CAP_PROP_FPS)))\n","\n","      img_path = os.path.join(output_path[:-7] ,\"images\")\n","      img_path_ls.append(img_path)\n","      # 出力ディレクトリが存在しない場合、作成します。\n","      if not os.path.exists(img_path):\n","          os.makedirs(img_path)\n","\n","      while cap.isOpened():\n","        torch.cuda.empty_cache()\n","        # del variables\n","        gc.collect()\n","\n","        torch.cuda.memory_summary(device=None, abbreviated=False)\n","        ret, frame = cap.read()\n","\n","        if not ret:\n","          #print(\"no image\")\n","          break\n","\n","        image_pil = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n","\n","        masks, boxes, phrases, logits = model.predict(image_pil, text_prompt)\n","\n","        if len(masks) == 0:\n","          print(f\"No objects of the '{text_prompt}' prompt detected in the image.\")\n","        else:\n","\n","          ## Convert masks to numpy arrays\n","          masks_np = [mask.squeeze().cpu().numpy() for mask in masks]\n","\n","          # # Display the original image and masks side by side\n","          # display_image_with_masks(image_pil, masks_np)\n","\n","          # # Display the image with bounding boxes and confidence scores\n","          # display_image_with_boxes(image_pil, boxes, logits)\n","\n","          # # # Save the masks\n","          # # for i, mask_np in enumerate(masks_np):\n","          # #     mask_path = f\"{os.getcwd()}/image_mask_{i+1}.png\"\n","          # #     save_mask(mask_np, mask_path)\n","\n","          # # Print the bounding boxes, phrases, and logits\n","          # print_bounding_boxes(boxes)\n","          # print_detected_phrases(phrases)\n","          # print_logits(logits)\n","\n","\n","          ## 審判以外の予測を取り出す\n","          # print(phrases)\n","          player_label_idx = []\n","          for idx, phrase in enumerate(phrases):\n","            if not ('the yellow floor the yellow floor' in phrase or \"the yellow floor\" in phrase):\n","\n","              player_label_idx.append(idx)\n","\n","          # print(player_label_idx)\n","          masks_np = [masks_np[idx] for idx in player_label_idx]\n","          # display_image_with_masks(image_pil, masks_np)\n","\n","          # ## bboxの座標が最も遠いマスクを除外する\n","          # boxes = [list(box.squeeze().cpu().numpy()) for box in boxes]\n","          # print(boxes)\n","          # _, farthest_box_index = filter_farthest_box(boxes)\n","          # masks_np = [masks_np[idx] for idx in range(len(boxes)) if idx != farthest_box_index]\n","\n","          # display_image_with_masks(image_pil, masks_np)\n","\n","\n","          ## 全てのマスクを合わせる\n","          masks_np = sum(masks_np).astype(bool)\n","\n","          masks_np = cv2.cvtColor(masks_np.astype(np.uint8) * 255, cv2.COLOR_GRAY2BGR)\n","\n","          # masks_np = np.concatenate([masks_np, masks_np, masks_np])\n","          frame = cv2.cvtColor(np.array(image_pil), cv2.COLOR_RGB2BGR)\n","\n","          result_img = cv2.bitwise_and(frame, masks_np)  #切り向き画像\n","          #result_img = cv2.cvtColor(masks_np, cv2.COLOR_BGR2GRAY)\n","\n","          result_path = os.path.join(img_path, f\"{str(count + 1).zfill(4)}.png\")\n","          cv2.imwrite(result_path, result_img)\n","          count += 1\n","\n","        img_path_ls.append(img_path)\n","  except (requests.exceptions.RequestException, IOError) as e:\n","    print(f\"Error: {e}\")\n"]},{"cell_type":"markdown","metadata":{"id":"kL0OC1u7MBBj"},"source":["xmem"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YqMerFYUMDBT"},"outputs":[],"source":["import locale\n","locale.getpreferredencoding = lambda: \"UTF-8\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1701851835525,"user":{"displayName":"盛拓矢","userId":"00009370238873879878"},"user_tz":-540},"id":"oYzkLxBBMKZp","outputId":"ccf6f689-b611-449a-f0cc-38299fbba0f9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Wed Dec  6 08:37:14 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  NVIDIA A100-SXM...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   28C    P0    42W / 400W |      0MiB / 40960MiB |      0%      Default |\n","|                               |                      |             Disabled |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n","Using GPU\n"]}],"source":["\n","!nvidia-smi\n","\n","import torch\n","\n","if torch.cuda.is_available():\n","  print('Using GPU')\n","  device = 'cuda'\n","else:\n","  print('CUDA not available. Please connect to a GPU instance if possible.')\n","  device = 'cpu'"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21530,"status":"ok","timestamp":1701851857051,"user":{"displayName":"盛拓矢","userId":"00009370238873879878"},"user_tz":-540},"id":"FZTEdgPJMLxw","outputId":"811214df-1b7d-408d-a43e-f589fb4d97c1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'XMem'...\n","remote: Enumerating objects: 596, done.\u001b[K\n","remote: Counting objects: 100% (324/324), done.\u001b[K\n","remote: Compressing objects: 100% (128/128), done.\u001b[K\n","remote: Total 596 (delta 233), reused 210 (delta 195), pack-reused 272\u001b[K\n","Receiving objects: 100% (596/596), 265.06 KiB | 3.05 MiB/s, done.\n","Resolving deltas: 100% (345/345), done.\n","/content/XMem\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.8.0.76)\n","Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.26.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.2)\n","Collecting git+https://github.com/cheind/py-thin-plate-spline (from -r requirements.txt (line 4))\n","  Cloning https://github.com/cheind/py-thin-plate-spline to /tmp/pip-req-build-rbien1c9\n","  Running command git clone --filter=blob:none --quiet https://github.com/cheind/py-thin-plate-spline /tmp/pip-req-build-rbien1c9\n","  Resolved https://github.com/cheind/py-thin-plate-spline to commit f6995795397118b7d0ac01aecd3f39ffbfad9dee\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: progressbar2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (4.2.0)\n","Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (4.6.6)\n","Collecting gitpython (from -r requirements.txt (line 3))\n","  Downloading GitPython-3.1.40-py3-none-any.whl (190 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting hickle (from -r requirements.txt (line 5))\n","  Downloading hickle-5.0.2-py3-none-any.whl (107 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.9/107.9 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (2.14.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (1.26.2)\n","Requirement already satisfied: python-utils>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from progressbar2->-r requirements.txt (line 1)) (3.8.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown->-r requirements.txt (line 2)) (3.13.1)\n","Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown->-r requirements.txt (line 2)) (2.31.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown->-r requirements.txt (line 2)) (1.16.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown->-r requirements.txt (line 2)) (4.66.1)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown->-r requirements.txt (line 2)) (4.11.2)\n","Collecting gitdb<5,>=4.0.1 (from gitpython->-r requirements.txt (line 3))\n","  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: torch>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from thinplate==1.0.0->-r requirements.txt (line 4)) (2.1.0+cu118)\n","Requirement already satisfied: torchvision>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from thinplate==1.0.0->-r requirements.txt (line 4)) (0.16.0+cu118)\n","Requirement already satisfied: h5py>=2.10.0 in /usr/local/lib/python3.10/dist-packages (from hickle->-r requirements.txt (line 5)) (3.9.0)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 6)) (1.4.0)\n","Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 6)) (1.59.3)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 6)) (2.17.3)\n","Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 6)) (1.0.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 6)) (3.5.1)\n","Requirement already satisfied: protobuf>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 6)) (3.20.3)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 6)) (67.7.2)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 6)) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 6)) (3.0.1)\n","Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython->-r requirements.txt (line 3))\n","  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 6)) (5.3.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 6)) (0.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 6)) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard->-r requirements.txt (line 6)) (1.3.1)\n","Requirement already satisfied: typing-extensions>3.10.0.2 in /usr/local/lib/python3.10/dist-packages (from python-utils>=3.0.0->progressbar2->-r requirements.txt (line 1)) (4.8.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown->-r requirements.txt (line 2)) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown->-r requirements.txt (line 2)) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown->-r requirements.txt (line 2)) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown->-r requirements.txt (line 2)) (2023.11.17)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->thinplate==1.0.0->-r requirements.txt (line 4)) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->thinplate==1.0.0->-r requirements.txt (line 4)) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->thinplate==1.0.0->-r requirements.txt (line 4)) (3.1.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->thinplate==1.0.0->-r requirements.txt (line 4)) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->thinplate==1.0.0->-r requirements.txt (line 4)) (2.1.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.2.1->thinplate==1.0.0->-r requirements.txt (line 4)) (9.3.0)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard->-r requirements.txt (line 6)) (2.1.3)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown->-r requirements.txt (line 2)) (2.5)\n","Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown->-r requirements.txt (line 2)) (1.7.1)\n","Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 6)) (0.5.1)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard->-r requirements.txt (line 6)) (3.2.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=0.4.0->thinplate==1.0.0->-r requirements.txt (line 4)) (1.3.0)\n","Building wheels for collected packages: thinplate\n","  Building wheel for thinplate (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for thinplate: filename=thinplate-1.0.0-py3-none-any.whl size=6701 sha256=722fac5bb75260a892f5c537e2dc526921bb48b465b4b6b5eed57cd888a785e9\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-f2nqgsaz/wheels/3e/f3/84/5780510c4585ad00e08b99a8ae29904185f037823b777af27c\n","Successfully built thinplate\n","Installing collected packages: smmap, hickle, gitdb, gitpython, thinplate\n","Successfully installed gitdb-4.0.11 gitpython-3.1.40 hickle-5.0.2 smmap-5.0.1 thinplate-1.0.0\n"]}],"source":["!git clone https://github.com/hkchengrex/XMem.git\n","%cd XMem\n","!pip install opencv-python\n","!pip install -U numpy\n","!pip install -r requirements.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1393,"status":"ok","timestamp":1701851858441,"user":{"displayName":"盛拓矢","userId":"00009370238873879878"},"user_tz":-540},"id":"_bNTIv3FMWbH","outputId":"2b30e4e7-6dd9-4b19-b2ae-8c8b5f00a401"},"outputs":[{"output_type":"stream","name":"stdout","text":["--2023-12-06 08:37:36--  https://github.com/hkchengrex/XMem/releases/download/v1.0/XMem.pth\n","Resolving github.com (github.com)... 140.82.113.4\n","Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/511262077/ea2968ee-04ab-4dee-8596-03319e8c7e9f?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20231206%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20231206T083737Z&X-Amz-Expires=300&X-Amz-Signature=fa0bed70c5ccb0ee62aa13a079e1c5ec6ac60323fe7412a5c44e0ad9160404bf&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=511262077&response-content-disposition=attachment%3B%20filename%3DXMem.pth&response-content-type=application%2Foctet-stream [following]\n","--2023-12-06 08:37:37--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/511262077/ea2968ee-04ab-4dee-8596-03319e8c7e9f?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20231206%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20231206T083737Z&X-Amz-Expires=300&X-Amz-Signature=fa0bed70c5ccb0ee62aa13a079e1c5ec6ac60323fe7412a5c44e0ad9160404bf&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=511262077&response-content-disposition=attachment%3B%20filename%3DXMem.pth&response-content-type=application%2Foctet-stream\n","Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 249026057 (237M) [application/octet-stream]\n","Saving to: ‘./saves/XMem.pth’\n","\n","XMem.pth            100%[===================>] 237.49M   281MB/s    in 0.8s    \n","\n","2023-12-06 08:37:37 (281 MB/s) - ‘./saves/XMem.pth’ saved [249026057/249026057]\n","\n"]}],"source":["# Download the pretrained model\n","!wget -P ./saves/ https://github.com/hkchengrex/XMem/releases/download/v1.0/XMem.pth"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8170,"status":"ok","timestamp":1701851866608,"user":{"displayName":"盛拓矢","userId":"00009370238873879878"},"user_tz":-540},"id":"cI38hbJlMkf_","outputId":"3022df00-d4e7-4a13-b9a9-e1e4efba5f69"},"outputs":[{"output_type":"stream","name":"stdout","text":["Hyperparameters read from the model weights: C^k=64, C^v=512, C^h=64\n","Single object mode: False\n"]},{"output_type":"stream","name":"stderr","text":["Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-19c8e357.pth\n","100%|██████████| 97.8M/97.8M [00:00<00:00, 292MB/s]\n","Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth\n","100%|██████████| 44.7M/44.7M [00:00<00:00, 221MB/s]\n"]}],"source":["# Basic setup\n","import os\n","from os import path\n","from argparse import ArgumentParser\n","import shutil\n","\n","import torch\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader\n","import numpy as np\n","from PIL import Image\n","\n","from inference.data.test_datasets import LongTestDataset, DAVISTestDataset, YouTubeVOSTestDataset\n","from inference.data.mask_mapper import MaskMapper\n","from model.network import XMem\n","from inference.inference_core import InferenceCore\n","\n","from progressbar import progressbar\n","\n","torch.set_grad_enabled(False)\n","\n","# default configuration\n","config = {\n","    'top_k': 30,\n","    'mem_every': 5,\n","    'deep_update_every': -1,\n","    'enable_long_term': True,\n","    'enable_long_term_count_usage': True,\n","    'num_prototypes': 128,\n","    'min_mid_term_frames': 5,\n","    'max_mid_term_frames': 10,\n","    'max_long_term_elements': 10000,\n","}\n","\n","network = XMem(config, './saves/XMem.pth').eval().to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PCqgwy8hRD-P"},"outputs":[],"source":["from numpy.ma.core import count\n","import cv2\n","from inference.interact.interactive_utils import image_to_torch, index_numpy_to_one_hot_torch, torch_prob_to_numpy_mask, overlay_davis\n","\n","\n","def xmem(video_path, masks_np, video_name, str_dir):\n","  num_objects = len(np.unique(masks_np)) - 1\n","  first_frame = 0\n","  torch.cuda.empty_cache()\n","\n","  processor = InferenceCore(network, config=config)\n","  processor.set_all_labels(range(1, num_objects+1)) # consecutive labels\n","  cap = cv2.VideoCapture(video_path)\n","  # フレームの総数\n","  totalframecount = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) - first_frame\n","  print(totalframecount)\n","  #指定したフレームをセットする\n","  cap.set(cv2.CAP_PROP_POS_FRAMES, first_frame)\n","\n","  # You can change these two numbers\n","  frames_to_propagate = totalframecount\n","  visualize_every = 200\n","\n","  current_frame_index = 0\n","  #count_value = 0\n","  img_save_dir = str_dir\n","  # 出力ディレクトリが存在しない場合、作成します。\n","  if not os.path.exists(img_save_dir):\n","      os.makedirs(img_save_dir)\n","\n","  img_path = os.path.join(img_save_dir ,\"video\", [file_name for file_name in os.listdir(os.getcwd()) if file_name[-6:-4] == \"_2\"][video_name][:-6], \"object_removal/images\")\n","\n","  masks_path = os.path.join(img_save_dir, \"video\", [file_name for file_name in os.listdir(os.getcwd()) if file_name[-6:-4] == \"_2\"][video_name][:-6], \"object_removal/masks\")\n","  output_path = os.path.join(img_save_dir, \"video\", [file_name for file_name in os.listdir(os.getcwd()) if file_name[-6:-4] == \"_2\"][video_name][:-6], \"result\")\n","\n","  # 出力ディレクトリが存在しない場合、作成します。\n","  if not os.path.exists(img_path):\n","      os.makedirs(img_path)\n","\n","  # 出力ディレクトリが存在しない場合、作成します。\n","  if not os.path.exists(masks_path):\n","      os.makedirs(masks_path)\n","  do_system(f\"yes | ffmpeg -i '{video_path}' '{img_path}/%04d.png'\")\n","\n","  with torch.cuda.amp.autocast(enabled=True):\n","    while (cap.isOpened()):\n","\n","      # load frame-by-frame\n","      _, frame = cap.read()\n","      if frame is None or current_frame_index > frames_to_propagate:\n","        break\n","\n","      # convert numpy array to pytorch tensor format\n","      frame_torch, _ = image_to_torch(frame, device=device)\n","      if current_frame_index == 0:\n","        # initialize with the mask\n","        mask_torch = index_numpy_to_one_hot_torch(masks_np, num_objects+1).to(device)\n","        # the background mask is not fed into the model\n","        prediction = processor.step(frame_torch, mask_torch[1:])\n","      else:\n","        # propagate only\n","        prediction = processor.step(frame_torch)\n","\n","      # argmax, convert to numpy\n","      prediction = torch_prob_to_numpy_mask(prediction)\n","\n","      if current_frame_index % visualize_every == 1:\n","        visualization = overlay_davis(frame, prediction)\n","        # display(Image.fromarray(visualization))\n","\n","        #cv2_imshow(visualization)\n","      # #動画にフレーム数を表示する\n","      # frame = cv2.putText(frame, f'{count_value}', org = (50, 50), fontFace = cv2.FONT_HERSHEY_PLAIN, fontScale = 1, color = (255, 255, 255), thickness = 1, lineType = cv2.LINE_AA)\n","      # writer.write(frame)\n","      # count_value += 1\n","      result_path = os.path.join(masks_path ,f\"{str(current_frame_index + 1).zfill(4)}.png\")\n","      cv2.imwrite(result_path, prediction*255)\n","      current_frame_index += 1\n","    cap.release()\n","    return img_path, masks_path, output_path"]},{"cell_type":"markdown","metadata":{"id":"Kt8-YWLrd-Av"},"source":[" ProPainter"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10326,"status":"ok","timestamp":1701851876912,"user":{"displayName":"盛拓矢","userId":"00009370238873879878"},"user_tz":-540},"id":"HZfVkUmTeArR","outputId":"fd860e12-db2d-4898-aecf-f2ee50ba9f4f"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n","Cloning into 'ProPainter'...\n","remote: Enumerating objects: 593, done.\u001b[K\n","remote: Counting objects: 100% (112/112), done.\u001b[K\n","remote: Compressing objects: 100% (81/81), done.\u001b[K\n","remote: Total 593 (delta 30), reused 92 (delta 24), pack-reused 481\u001b[K\n","Receiving objects: 100% (593/593), 55.21 MiB | 54.73 MiB/s, done.\n","Resolving deltas: 100% (83/83), done.\n","/content/ProPainter\n","Collecting av (from -r requirements.txt (line 1))\n","  Downloading av-11.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.9/32.9 MB\u001b[0m \u001b[31m50.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: addict in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (2.4.0)\n","Collecting einops (from -r requirements.txt (line 3))\n","  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (0.18.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (1.26.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (1.11.4)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (4.8.0.76)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (3.7.1)\n","Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (0.19.3)\n","Requirement already satisfied: torch>=1.7.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (2.1.0+cu118)\n","Requirement already satisfied: torchvision>=0.8.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 11)) (0.16.0+cu118)\n","Requirement already satisfied: imageio-ffmpeg in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 12)) (0.4.9)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 13)) (6.0.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 14)) (2.31.0)\n","Requirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 15)) (0.9.12)\n","Requirement already satisfied: yapf in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 16)) (0.40.2)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 8)) (1.2.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 8)) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 8)) (4.45.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 8)) (1.4.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 8)) (23.2)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 8)) (9.3.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 8)) (3.1.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 8)) (2.8.2)\n","Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image->-r requirements.txt (line 9)) (3.2.1)\n","Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->-r requirements.txt (line 9)) (2.31.6)\n","Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image->-r requirements.txt (line 9)) (2023.9.26)\n","Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->-r requirements.txt (line 9)) (1.5.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->-r requirements.txt (line 10)) (3.13.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->-r requirements.txt (line 10)) (4.8.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->-r requirements.txt (line 10)) (1.12)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->-r requirements.txt (line 10)) (3.1.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->-r requirements.txt (line 10)) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->-r requirements.txt (line 10)) (2.1.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from imageio-ffmpeg->-r requirements.txt (line 12)) (67.7.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->-r requirements.txt (line 14)) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->-r requirements.txt (line 14)) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->-r requirements.txt (line 14)) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->-r requirements.txt (line 14)) (2023.11.17)\n","Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from timm->-r requirements.txt (line 15)) (0.16.4)\n","Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm->-r requirements.txt (line 15)) (0.4.1)\n","Requirement already satisfied: importlib-metadata>=6.6.0 in /usr/local/lib/python3.10/dist-packages (from yapf->-r requirements.txt (line 16)) (6.8.0)\n","Requirement already satisfied: platformdirs>=3.5.1 in /usr/local/lib/python3.10/dist-packages (from yapf->-r requirements.txt (line 16)) (4.0.0)\n","Requirement already satisfied: tomli>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from yapf->-r requirements.txt (line 16)) (2.0.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=6.6.0->yapf->-r requirements.txt (line 16)) (3.17.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->-r requirements.txt (line 8)) (1.16.0)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm->-r requirements.txt (line 15)) (4.66.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.7.1->-r requirements.txt (line 10)) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.7.1->-r requirements.txt (line 10)) (1.3.0)\n","Installing collected packages: einops, av\n","Successfully installed av-11.0.0 einops-0.7.0\n"]}],"source":["%cd /content\n","!git clone https://github.com/sczhou/ProPainter.git\n","\n","%cd ProPainter\n","# install python dependencies\n","!pip install -r requirements.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2932,"status":"ok","timestamp":1701851879819,"user":{"displayName":"盛拓矢","userId":"00009370238873879878"},"user_tz":-540},"id":"l-kQWWS1eTlG","outputId":"57a6ce8a-e56c-43d4-fa80-10b653e39d21"},"outputs":[{"output_type":"stream","name":"stdout","text":["wget: option requires an argument -- 'P'\n","wget: missing URL\n","Usage: wget [OPTION]... [URL]...\n","\n","Try `wget --help' for more options.\n","--2023-12-06 08:37:56--  https://github.com/sczhou/ProPainter/releases/download/v0.1.0/i3d_rgb_imagenet.pt\n","Resolving github.com (github.com)... 140.82.114.3\n","Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/685984512/2050cfd9-4052-4f78-af98-f06861465a76?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20231206%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20231206T083756Z&X-Amz-Expires=300&X-Amz-Signature=80b90dcc8a72bfdf419616cbdb694e9f4e3af586602aa82579cf372320453114&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=685984512&response-content-disposition=attachment%3B%20filename%3Di3d_rgb_imagenet.pt&response-content-type=application%2Foctet-stream [following]\n","--2023-12-06 08:37:56--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/685984512/2050cfd9-4052-4f78-af98-f06861465a76?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20231206%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20231206T083756Z&X-Amz-Expires=300&X-Amz-Signature=80b90dcc8a72bfdf419616cbdb694e9f4e3af586602aa82579cf372320453114&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=685984512&response-content-disposition=attachment%3B%20filename%3Di3d_rgb_imagenet.pt&response-content-type=application%2Foctet-stream\n","Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.110.133, ...\n","Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.111.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 50883138 (49M) [application/octet-stream]\n","Saving to: ‘weights/i3d_rgb_imagenet.pt’\n","\n","i3d_rgb_imagenet.pt 100%[===================>]  48.53M   127MB/s    in 0.4s    \n","\n","2023-12-06 08:37:57 (127 MB/s) - ‘weights/i3d_rgb_imagenet.pt’ saved [50883138/50883138]\n","\n","--2023-12-06 08:37:57--  https://github.com/sczhou/ProPainter/releases/download/v0.1.0/ProPainter.pth\n","Resolving github.com (github.com)... 140.82.113.3\n","Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/685984512/0342f497-c678-4e11-936b-5bb4cebe6c70?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20231206%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20231206T083757Z&X-Amz-Expires=300&X-Amz-Signature=cbc462b360c5afe2bb737fb659b440064fe68ad99b0e721ed07ee3ba9e448279&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=685984512&response-content-disposition=attachment%3B%20filename%3DProPainter.pth&response-content-type=application%2Foctet-stream [following]\n","--2023-12-06 08:37:57--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/685984512/0342f497-c678-4e11-936b-5bb4cebe6c70?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20231206%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20231206T083757Z&X-Amz-Expires=300&X-Amz-Signature=cbc462b360c5afe2bb737fb659b440064fe68ad99b0e721ed07ee3ba9e448279&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=685984512&response-content-disposition=attachment%3B%20filename%3DProPainter.pth&response-content-type=application%2Foctet-stream\n","Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 157780510 (150M) [application/octet-stream]\n","Saving to: ‘weights/ProPainter.pth’\n","\n","ProPainter.pth      100%[===================>] 150.47M   276MB/s    in 0.5s    \n","\n","2023-12-06 08:37:58 (276 MB/s) - ‘weights/ProPainter.pth’ saved [157780510/157780510]\n","\n","--2023-12-06 08:37:58--  https://github.com/sczhou/ProPainter/releases/download/v0.1.0/raft-things.pth\n","Resolving github.com (github.com)... 140.82.113.4\n","Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/685984512/b7614d2a-824a-4ce2-b78b-d4feb274a3f0?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20231206%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20231206T083758Z&X-Amz-Expires=300&X-Amz-Signature=558de2261553ad3325629e7210c1640f1f80f25f399e3149aa3977f2be46df97&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=685984512&response-content-disposition=attachment%3B%20filename%3Draft-things.pth&response-content-type=application%2Foctet-stream [following]\n","--2023-12-06 08:37:58--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/685984512/b7614d2a-824a-4ce2-b78b-d4feb274a3f0?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20231206%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20231206T083758Z&X-Amz-Expires=300&X-Amz-Signature=558de2261553ad3325629e7210c1640f1f80f25f399e3149aa3977f2be46df97&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=685984512&response-content-disposition=attachment%3B%20filename%3Draft-things.pth&response-content-type=application%2Foctet-stream\n","Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n","Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.110.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 21108000 (20M) [application/octet-stream]\n","Saving to: ‘weights/raft-things.pth’\n","\n","raft-things.pth     100%[===================>]  20.13M  --.-KB/s    in 0.1s    \n","\n","2023-12-06 08:37:58 (161 MB/s) - ‘weights/raft-things.pth’ saved [21108000/21108000]\n","\n","--2023-12-06 08:37:58--  https://github.com/sczhou/ProPainter/releases/download/v0.1.0/recurrent_flow_completion.pth\n","Resolving github.com (github.com)... 140.82.113.4\n","Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/685984512/f84f5e52-81b7-412c-9bd3-0625ea40fa8b?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20231206%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20231206T083759Z&X-Amz-Expires=300&X-Amz-Signature=69e8b6744461f44e407a5a9207162a26c821e3c4b557bd002b047b2c8dc0fbd7&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=685984512&response-content-disposition=attachment%3B%20filename%3Drecurrent_flow_completion.pth&response-content-type=application%2Foctet-stream [following]\n","--2023-12-06 08:37:59--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/685984512/f84f5e52-81b7-412c-9bd3-0625ea40fa8b?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20231206%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20231206T083759Z&X-Amz-Expires=300&X-Amz-Signature=69e8b6744461f44e407a5a9207162a26c821e3c4b557bd002b047b2c8dc0fbd7&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=685984512&response-content-disposition=attachment%3B%20filename%3Drecurrent_flow_completion.pth&response-content-type=application%2Foctet-stream\n","Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\n","Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.109.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 20348681 (19M) [application/octet-stream]\n","Saving to: ‘weights/recurrent_flow_completion.pth’\n","\n","recurrent_flow_comp 100%[===================>]  19.41M  --.-KB/s    in 0.1s    \n","\n","2023-12-06 08:37:59 (151 MB/s) - ‘weights/recurrent_flow_completion.pth’ saved [20348681/20348681]\n","\n"]}],"source":["!wget -P weights/ https://github.com/sczhou/ProPainter/releases/download/v0.1.0/cutie-base-mega.pth -P\n","!wget -P weights/ https://github.com/sczhou/ProPainter/releases/download/v0.1.0/i3d_rgb_imagenet.pt\n","!wget -P weights/ https://github.com/sczhou/ProPainter/releases/download/v0.1.0/ProPainter.pth\n","!wget -P weights/ https://github.com/sczhou/ProPainter/releases/download/v0.1.0/raft-things.pth\n","!wget -P weights/ https://github.com/sczhou/ProPainter/releases/download/v0.1.0/recurrent_flow_completion.pth"]},{"cell_type":"markdown","metadata":{"id":"6fAUdOJKejLS"},"source":["# 審判を省く"]},{"cell_type":"markdown","metadata":{"id":"6n_2mKndg_XY"},"source":["## 2018"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["4a4c332e0f4c45ad9db6809706776ad9","324009fa4a984f93889426b4056a8371","6da26bf6e6504a25afae58d80485470c","4c05b7f8ea144f50b656cf99380779bd","21a93515b46d4c40b8298a3e388eeab8","1288bc8a1dff4665bc9dc588c3f3ef71","d3b02a9a644f4320967adf8764902db8","0a52ccc67e7c4f1486283482afd03388","afb2f442cb7d499094de6727cb421748","043df64206a24d07b404c75f2bf04030","88a9a408c3974df58e85f98f6d373e83","facccd8f261b428b968f843450a50103","ba6a5383e9b34036bd18e0e874db0fe3","cb2368e4d433496893af80c0bba7d0b5","f0ea551633d74aec94e219a60596b7e6","46cd43c8662146bda602965accf55f0e","51493f9945f54831947c61dd86abf5b6","f13e4f66f2d04c7c93b30a1d01145de6","b0c49c67e1344c3c8ab20006240ee840","046aaf68d02f4e84b1aec66fb4f930a4","bcd0156a0c744a54b1e15b9e387c59f8","67b61288f1b24734997a600ca0b99093","4e9cf9f3290841748ddbd4613340e2bf","f4840e6c5df74b049fab512e0682debf","025f064f2d4c4bce9d465fa605106bb0","4648860d9cc14e12855f92ef0e8220a0","0e44ed3b6ba64025bc757d0bba61ed30","7e01add16d3f4172a7ee5bfd2b555a65","28ad258b155a4358922ef86f7f1ac4be","98bf6bfa9307414f9fa7e2f947298660","f06f1e386e4b4044b0a549efb356a25d","034cd10ad6ee413fa80cc5b7d87f43d9","4739f58f4c3940cd95ea1ef8406cbbb0","34c33ee5d04141d4ac0101aae533e3ba","dbb068862ec14008899b5e45046c21fe","e11120c204a24f3c9730e25873b768df","36ae4b62e90947f8be33a2664330951e","e9b7725bbadc43fa80ffab91834c4296","dd3d8cdab42c4e0eb2a79aaa58d927b4","50ad5f2b5a224763adcbc0ba589fd9fe","33e8628751b9418ab2812e873e21970c","a7d0cde641d84ecea043b17ba1322efb","78ea65ecf7454276be9daa72873dfc65","da821e5bfdec41b78ca6fe6d3575f84f","2c69ab15a33c431d914ccb48fcc6cef0","e21930f801764a019e9045e7d97b89d6","3e34e9bc477a469992bbb2d6f1428768","a347962c3aff474ea254340f671e4c3e","59ec03fe560c4f458f2c462371933978","61627101af9144f5a86c06eec174835a","dfcf23be7b5c4566abffc2140b991bde","e4794e14a73f47faa155a2cd46340680","384f43c372f2441f9b5cfe7024b782a3","7f240899b56d4119a315fb2d83eb409a","58dae029e71446c480d96ceccf1bb8e4","f7a0dbc2911c442ba8b7d32832803189","ec7418d2c8ac4467b7e567611044d816","5b978e529f784680b9294593a90433a0","8db1f8e1951842b3b7df923771f1e46a","437aa8d6f64340d49168e6771ed2faab","ec87e04430484432810977f16acb3f5e","8d050e19e5b1492cbe6c22a03582b0bd","ee62f1467ed54a838e7f5513e0ac692d","e59cc7c51db14c0088bb81c9c97c9f7f","019cb61c5b464b83bb4e4b02db51f0b9","13855d6389844d93a91830cc8efbdbe0","970e397cf8e2473a96b5308100eaf9cd","28946126c46141bbb15f7c3939d1f334","90427614a762429fb1b169a307d2617b","99bbf751a2524812ae502cb3fe20ccab","bf5cf79e157b47f4a63b924dd6e3e061","608397eb6fbd4eb8ba31cf96400479d6","7537404796be490fbcfb560973d80ee7","1297f14e5331414d92d5a6d1b6dc67bb","6be379671c7f406e9b754c208ca1b2c5","514672913ef048fcb38f65f7bccc083b","9aef13e9c47a424c8bd35b6cf38b1710"]},"id":"1X_GDyDWWODI","executionInfo":{"status":"error","timestamp":1701856694987,"user_tz":-540,"elapsed":484676,"user":{"displayName":"盛拓矢","userId":"00009370238873879878"}},"outputId":"69aa9876-c3d1-4e1b-f278-acc465f93d5e"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4a4c332e0f4c45ad9db6809706776ad9","version_major":2,"version_minor":0},"text/plain":["Downloading (…)ingDINO_SwinB.cfg.py:   0%|          | 0.00/1.01k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["final text_encoder_type: bert-base-uncased\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"facccd8f261b428b968f843450a50103","version_major":2,"version_minor":0},"text/plain":["Downloading tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4e9cf9f3290841748ddbd4613340e2bf","version_major":2,"version_minor":0},"text/plain":["Downloading config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"34c33ee5d04141d4ac0101aae533e3ba","version_major":2,"version_minor":0},"text/plain":["Downloading vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2c69ab15a33c431d914ccb48fcc6cef0","version_major":2,"version_minor":0},"text/plain":["Downloading tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f7a0dbc2911c442ba8b7d32832803189","version_major":2,"version_minor":0},"text/plain":["Downloading model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"970e397cf8e2473a96b5308100eaf9cd","version_major":2,"version_minor":0},"text/plain":["Downloading (…)no_swinb_cogcoor.pth:   0%|          | 0.00/938M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Model loaded from /root/.cache/huggingface/hub/models--ShilongLiu--GroundingDINO/snapshots/a94c9b567a2a374598f05c584e96798a170c56fb/groundingdino_swinb_cogcoor.pth \n"," => _IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Downloading: \"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\" to /root/.cache/torch/hub/checkpoints/sam_vit_h_4b8939.pth\n","100%|██████████| 2.39G/2.39G [00:07<00:00, 336MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["/content/drive/.shortcut-targets-by-id/1wERFTJ3tk-moekRG-4KtAeT8uJewI1Y3/データ/3second_videos/game_over_51/video/2019WCT_63_SF_AGBEGNENOU Clarisse_FRA_FRANSSEN Juul_NED\n","73\n","==== running: yes | ffmpeg -i '/content/drive/.shortcut-targets-by-id/1wERFTJ3tk-moekRG-4KtAeT8uJewI1Y3/データ/3second_videos/game_over_51/2019WCT_63_SF_AGBEGNENOU Clarisse_FRA_FRANSSEN Juul_NED_2.mp4' '/content/drive/MyDrive/Colab Notebooks/2023年度スポコン/データ/3second_videos/game_over_51/video/2019WCT_63_SF_AGBEGNENOU Clarisse_FRA_FRANSSEN Juul_NED/object_removal/images/%04d.png'\n","Pretrained flow completion model has loaded...\n","Pretrained ProPainter has loaded...\n","Network [InpaintGenerator] was created. Total number of parameters: 39.4 million. To see the architecture, do print(network).\n","\n","Processing: images [73 frames]...\n","100% 15/15 [00:06<00:00,  2.49it/s]\n","IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (640, 360) to (640, 368) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n","IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (640, 360) to (640, 368) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n","\n","All results are saved in /content/drive/MyDrive/Colab Notebooks/2023年度スポコン/データ/3second_videos/game_over_51/video/2019WCT_63_SF_AGBEGNENOU Clarisse_FRA_FRANSSEN Juul_NED/result/images\n","final text_encoder_type: bert-base-uncased\n","Model loaded from /root/.cache/huggingface/hub/models--ShilongLiu--GroundingDINO/snapshots/a94c9b567a2a374598f05c584e96798a170c56fb/groundingdino_swinb_cogcoor.pth \n"," => _IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n","/content/drive/MyDrive/Colab Notebooks/2023年度スポコン/データ/3second_videos/game_over_51/video/2019WCT_63_SF_AGBEGNENOU Clarisse_FRA_FRANSSEN Juul_NED/result/images/inpaint_out.mp4\n","/content/drive/.shortcut-targets-by-id/1wERFTJ3tk-moekRG-4KtAeT8uJewI1Y3/データ/3second_videos/game_over_51/video/2019WCT_66_P3_LOMBARDO Manuel_ITA_MINKOU Dzmitry_BLR\n","73\n","==== running: yes | ffmpeg -i '/content/drive/.shortcut-targets-by-id/1wERFTJ3tk-moekRG-4KtAeT8uJewI1Y3/データ/3second_videos/game_over_51/2019WCT_66_P3_LOMBARDO Manuel_ITA_MINKOU Dzmitry_BLR_2.mp4' '/content/drive/MyDrive/Colab Notebooks/2023年度スポコン/データ/3second_videos/game_over_51/video/2019WCT_66_P3_LOMBARDO Manuel_ITA_MINKOU Dzmitry_BLR/object_removal/images/%04d.png'\n","Pretrained flow completion model has loaded...\n","Pretrained ProPainter has loaded...\n","Network [InpaintGenerator] was created. Total number of parameters: 39.4 million. To see the architecture, do print(network).\n","\n","Processing: images [73 frames]...\n","100% 15/15 [00:06<00:00,  2.49it/s]\n","IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (640, 360) to (640, 368) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n","IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (640, 360) to (640, 368) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n","\n","All results are saved in /content/drive/MyDrive/Colab Notebooks/2023年度スポコン/データ/3second_videos/game_over_51/video/2019WCT_66_P3_LOMBARDO Manuel_ITA_MINKOU Dzmitry_BLR/result/images\n","final text_encoder_type: bert-base-uncased\n","Model loaded from /root/.cache/huggingface/hub/models--ShilongLiu--GroundingDINO/snapshots/a94c9b567a2a374598f05c584e96798a170c56fb/groundingdino_swinb_cogcoor.pth \n"," => _IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n","/content/drive/MyDrive/Colab Notebooks/2023年度スポコン/データ/3second_videos/game_over_51/video/2019WCT_66_P3_LOMBARDO Manuel_ITA_MINKOU Dzmitry_BLR/result/images/inpaint_out.mp4\n","/content/drive/.shortcut-targets-by-id/1wERFTJ3tk-moekRG-4KtAeT8uJewI1Y3/データ/3second_videos/game_over_51/video/2021WCB_70_P3_JAGER Hilde_NED_MATIC Barbara_CRO\n","73\n","==== running: yes | ffmpeg -i '/content/drive/.shortcut-targets-by-id/1wERFTJ3tk-moekRG-4KtAeT8uJewI1Y3/データ/3second_videos/game_over_51/2021WCB_70_P3_JAGER Hilde_NED_MATIC Barbara_CRO_2.mp4' '/content/drive/MyDrive/Colab Notebooks/2023年度スポコン/データ/3second_videos/game_over_51/video/2021WCB_70_P3_JAGER Hilde_NED_MATIC Barbara_CRO/object_removal/images/%04d.png'\n","Pretrained flow completion model has loaded...\n","Pretrained ProPainter has loaded...\n","Network [InpaintGenerator] was created. Total number of parameters: 39.4 million. To see the architecture, do print(network).\n","\n","Processing: images [73 frames]...\n","100% 15/15 [00:06<00:00,  2.50it/s]\n","IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (640, 360) to (640, 368) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n","IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (640, 360) to (640, 368) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n","\n","All results are saved in /content/drive/MyDrive/Colab Notebooks/2023年度スポコン/データ/3second_videos/game_over_51/video/2021WCB_70_P3_JAGER Hilde_NED_MATIC Barbara_CRO/result/images\n","final text_encoder_type: bert-base-uncased\n","Model loaded from /root/.cache/huggingface/hub/models--ShilongLiu--GroundingDINO/snapshots/a94c9b567a2a374598f05c584e96798a170c56fb/groundingdino_swinb_cogcoor.pth \n"," => _IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n","/content/drive/MyDrive/Colab Notebooks/2023年度スポコン/データ/3second_videos/game_over_51/video/2021WCB_70_P3_JAGER Hilde_NED_MATIC Barbara_CRO/result/images/inpaint_out.mp4\n","/content/drive/.shortcut-targets-by-id/1wERFTJ3tk-moekRG-4KtAeT8uJewI1Y3/データ/3second_videos/game_over_51/video/2018WCB_63_P2_TALACH Karolina_POL_GARCIA Estefania_ECU\n","73\n","==== running: yes | ffmpeg -i '/content/drive/.shortcut-targets-by-id/1wERFTJ3tk-moekRG-4KtAeT8uJewI1Y3/データ/3second_videos/game_over_51/2018WCB_63_P2_TALACH Karolina_POL_GARCIA Estefania_ECU_2.mp4' '/content/drive/MyDrive/Colab Notebooks/2023年度スポコン/データ/3second_videos/game_over_51/video/2018WCB_63_P2_TALACH Karolina_POL_GARCIA Estefania_ECU/object_removal/images/%04d.png'\n","Pretrained flow completion model has loaded...\n","Pretrained ProPainter has loaded...\n","Network [InpaintGenerator] was created. Total number of parameters: 39.4 million. To see the architecture, do print(network).\n","\n","Processing: images [73 frames]...\n","100% 15/15 [00:06<00:00,  2.45it/s]\n","IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (640, 360) to (640, 368) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n","IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (640, 360) to (640, 368) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n","\n","All results are saved in /content/drive/MyDrive/Colab Notebooks/2023年度スポコン/データ/3second_videos/game_over_51/video/2018WCB_63_P2_TALACH Karolina_POL_GARCIA Estefania_ECU/result/images\n","final text_encoder_type: bert-base-uncased\n","Model loaded from /root/.cache/huggingface/hub/models--ShilongLiu--GroundingDINO/snapshots/a94c9b567a2a374598f05c584e96798a170c56fb/groundingdino_swinb_cogcoor.pth \n"," => _IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n","/content/drive/MyDrive/Colab Notebooks/2023年度スポコン/データ/3second_videos/game_over_51/video/2018WCB_63_P2_TALACH Karolina_POL_GARCIA Estefania_ECU/result/images/inpaint_out.mp4\n","/content/drive/.shortcut-targets-by-id/1wERFTJ3tk-moekRG-4KtAeT8uJewI1Y3/データ/3second_videos/game_over_51/video/2021WCB_P78_P2_SOUZA Beatriz_BRA_FONTAINE Lea_FRA\n","90\n","==== running: yes | ffmpeg -i '/content/drive/.shortcut-targets-by-id/1wERFTJ3tk-moekRG-4KtAeT8uJewI1Y3/データ/3second_videos/game_over_51/2021WCB_P78_P2_SOUZA Beatriz_BRA_FONTAINE Lea_FRA_2.mp4' '/content/drive/MyDrive/Colab Notebooks/2023年度スポコン/データ/3second_videos/game_over_51/video/2021WCB_P78_P2_SOUZA Beatriz_BRA_FONTAINE Lea_FRA/object_removal/images/%04d.png'\n","Pretrained flow completion model has loaded...\n","Pretrained ProPainter has loaded...\n","Network [InpaintGenerator] was created. Total number of parameters: 39.4 million. To see the architecture, do print(network).\n","\n","Processing: images [90 frames]...\n","Traceback (most recent call last):\n","  File \"/content/ProPainter/inference_propainter.py\", line 319, in <module>\n","    flows_f, flows_b = fix_raft(frames[:,f-1:end_f], iters=args.raft_iter)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/content/ProPainter/model/modules/flow_comp_raft.py\", line 48, in forward\n","    _, gt_flows_forward = self.fix_raft(gtlf_1, gtlf_2, iters=iters, test_mode=True)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/content/ProPainter/RAFT/raft.py\", line 109, in forward\n","    corr_fn = CorrBlock(fmap1, fmap2, radius=self.args.corr_radius)\n","  File \"/content/ProPainter/RAFT/corr.py\", line 19, in __init__\n","    corr = CorrBlock.corr(fmap1, fmap2)\n","  File \"/content/ProPainter/RAFT/corr.py\", line 60, in corr\n","    return corr  / torch.sqrt(torch.tensor(dim).float())\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 7.82 GiB. GPU 0 has a total capacty of 39.56 GiB of which 2.53 GiB is free. Process 11291 has 24.28 GiB memory in use. Process 66593 has 12.75 GiB memory in use. Of the allocated memory 11.65 GiB is allocated by PyTorch, and 613.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","final text_encoder_type: bert-base-uncased\n","Model loaded from /root/.cache/huggingface/hub/models--ShilongLiu--GroundingDINO/snapshots/a94c9b567a2a374598f05c584e96798a170c56fb/groundingdino_swinb_cogcoor.pth \n"," => _IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n","/content/drive/MyDrive/Colab Notebooks/2023年度スポコン/データ/3second_videos/game_over_51/video/2021WCB_P78_P2_SOUZA Beatriz_BRA_FONTAINE Lea_FRA/result/images/inpaint_out.mp4\n","/content/drive/.shortcut-targets-by-id/1wERFTJ3tk-moekRG-4KtAeT8uJewI1Y3/データ/3second_videos/game_over_51/video/2018WCB_66_P2_CARGNIN Daniel_BRA_ZADRO Petar_BIH\n","73\n","==== running: yes | ffmpeg -i '/content/drive/.shortcut-targets-by-id/1wERFTJ3tk-moekRG-4KtAeT8uJewI1Y3/データ/3second_videos/game_over_51/2018WCB_66_P2_CARGNIN Daniel_BRA_ZADRO Petar_BIH_2.mp4' '/content/drive/MyDrive/Colab Notebooks/2023年度スポコン/データ/3second_videos/game_over_51/video/2018WCB_66_P2_CARGNIN Daniel_BRA_ZADRO Petar_BIH/object_removal/images/%04d.png'\n","Pretrained flow completion model has loaded...\n","Pretrained ProPainter has loaded...\n","Network [InpaintGenerator] was created. Total number of parameters: 39.4 million. To see the architecture, do print(network).\n","\n","Processing: images [73 frames]...\n","100% 15/15 [00:06<00:00,  2.44it/s]\n","IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (640, 360) to (640, 368) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n","IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (640, 360) to (640, 368) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n","\n","All results are saved in /content/drive/MyDrive/Colab Notebooks/2023年度スポコン/データ/3second_videos/game_over_51/video/2018WCB_66_P2_CARGNIN Daniel_BRA_ZADRO Petar_BIH/result/images\n","final text_encoder_type: bert-base-uncased\n","Model loaded from /root/.cache/huggingface/hub/models--ShilongLiu--GroundingDINO/snapshots/a94c9b567a2a374598f05c584e96798a170c56fb/groundingdino_swinb_cogcoor.pth \n"," => _IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n","/content/drive/MyDrive/Colab Notebooks/2023年度スポコン/データ/3second_videos/game_over_51/video/2018WCB_66_P2_CARGNIN Daniel_BRA_ZADRO Petar_BIH/result/images/inpaint_out.mp4\n","/content/drive/.shortcut-targets-by-id/1wERFTJ3tk-moekRG-4KtAeT8uJewI1Y3/データ/3second_videos/game_over_51/video/2018WCB_70_P1_SMAL Nataliya_UKR_TAEYMANS Roxane_BEL\n","73\n","==== running: yes | ffmpeg -i '/content/drive/.shortcut-targets-by-id/1wERFTJ3tk-moekRG-4KtAeT8uJewI1Y3/データ/3second_videos/game_over_51/2018WCB_70_P1_SMAL Nataliya_UKR_TAEYMANS Roxane_BEL_2.mp4' '/content/drive/MyDrive/Colab Notebooks/2023年度スポコン/データ/3second_videos/game_over_51/video/2018WCB_70_P1_SMAL Nataliya_UKR_TAEYMANS Roxane_BEL/object_removal/images/%04d.png'\n","Pretrained flow completion model has loaded...\n","Pretrained ProPainter has loaded...\n","Network [InpaintGenerator] was created. Total number of parameters: 39.4 million. To see the architecture, do print(network).\n","\n","Processing: images [73 frames]...\n","100% 15/15 [00:05<00:00,  2.51it/s]\n","IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (640, 360) to (640, 368) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n","IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (640, 360) to (640, 368) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n","\n","All results are saved in /content/drive/MyDrive/Colab Notebooks/2023年度スポコン/データ/3second_videos/game_over_51/video/2018WCB_70_P1_SMAL Nataliya_UKR_TAEYMANS Roxane_BEL/result/images\n","final text_encoder_type: bert-base-uncased\n","Model loaded from /root/.cache/huggingface/hub/models--ShilongLiu--GroundingDINO/snapshots/a94c9b567a2a374598f05c584e96798a170c56fb/groundingdino_swinb_cogcoor.pth \n"," => _IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n","/content/drive/MyDrive/Colab Notebooks/2023年度スポコン/データ/3second_videos/game_over_51/video/2018WCB_70_P1_SMAL Nataliya_UKR_TAEYMANS Roxane_BEL/result/images/inpaint_out.mp4\n","/content/drive/.shortcut-targets-by-id/1wERFTJ3tk-moekRG-4KtAeT8uJewI1Y3/データ/3second_videos/game_over_51/video/2019WCT_81_P1_TERADA Masayuki_THA_WRIGHT Arthur_USA\n","73\n","==== running: yes | ffmpeg -i '/content/drive/.shortcut-targets-by-id/1wERFTJ3tk-moekRG-4KtAeT8uJewI1Y3/データ/3second_videos/game_over_51/2019WCT_81_P1_TERADA Masayuki_THA_WRIGHT Arthur_USA_2.mp4' '/content/drive/MyDrive/Colab Notebooks/2023年度スポコン/データ/3second_videos/game_over_51/video/2019WCT_81_P1_TERADA Masayuki_THA_WRIGHT Arthur_USA/object_removal/images/%04d.png'\n","Pretrained flow completion model has loaded...\n","Pretrained ProPainter has loaded...\n","Network [InpaintGenerator] was created. Total number of parameters: 39.4 million. To see the architecture, do print(network).\n","\n","Processing: images [73 frames]...\n","100% 15/15 [00:06<00:00,  2.45it/s]\n","IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (640, 360) to (640, 368) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n","IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (640, 360) to (640, 368) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n","\n","All results are saved in /content/drive/MyDrive/Colab Notebooks/2023年度スポコン/データ/3second_videos/game_over_51/video/2019WCT_81_P1_TERADA Masayuki_THA_WRIGHT Arthur_USA/result/images\n","final text_encoder_type: bert-base-uncased\n","Model loaded from /root/.cache/huggingface/hub/models--ShilongLiu--GroundingDINO/snapshots/a94c9b567a2a374598f05c584e96798a170c56fb/groundingdino_swinb_cogcoor.pth \n"," => _IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n","/content/drive/MyDrive/Colab Notebooks/2023年度スポコン/データ/3second_videos/game_over_51/video/2019WCT_81_P1_TERADA Masayuki_THA_WRIGHT Arthur_USA/result/images/inpaint_out.mp4\n","/content/drive/.shortcut-targets-by-id/1wERFTJ3tk-moekRG-4KtAeT8uJewI1Y3/データ/3second_videos/game_over_51/video/2018WCB_100_P1_DEMYANENKO Viktor_KAZ_REMARENCO Ivan_UAE\n","73\n","==== running: yes | ffmpeg -i '/content/drive/.shortcut-targets-by-id/1wERFTJ3tk-moekRG-4KtAeT8uJewI1Y3/データ/3second_videos/game_over_51/2018WCB_100_P1_DEMYANENKO Viktor_KAZ_REMARENCO Ivan_UAE_2.mp4' '/content/drive/MyDrive/Colab Notebooks/2023年度スポコン/データ/3second_videos/game_over_51/video/2018WCB_100_P1_DEMYANENKO Viktor_KAZ_REMARENCO Ivan_UAE/object_removal/images/%04d.png'\n","Pretrained flow completion model has loaded...\n","Pretrained ProPainter has loaded...\n","Network [InpaintGenerator] was created. Total number of parameters: 39.4 million. To see the architecture, do print(network).\n","\n","Processing: images [73 frames]...\n","100% 15/15 [00:06<00:00,  2.45it/s]\n","IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (640, 360) to (640, 368) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n","IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (640, 360) to (640, 368) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n","\n","All results are saved in /content/drive/MyDrive/Colab Notebooks/2023年度スポコン/データ/3second_videos/game_over_51/video/2018WCB_100_P1_DEMYANENKO Viktor_KAZ_REMARENCO Ivan_UAE/result/images\n","final text_encoder_type: bert-base-uncased\n","Model loaded from /root/.cache/huggingface/hub/models--ShilongLiu--GroundingDINO/snapshots/a94c9b567a2a374598f05c584e96798a170c56fb/groundingdino_swinb_cogcoor.pth \n"," => _IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n","/content/drive/MyDrive/Colab Notebooks/2023年度スポコン/データ/3second_videos/game_over_51/video/2018WCB_100_P1_DEMYANENKO Viktor_KAZ_REMARENCO Ivan_UAE/result/images/inpaint_out.mp4\n","/content/drive/.shortcut-targets-by-id/1wERFTJ3tk-moekRG-4KtAeT8uJewI1Y3/データ/3second_videos/game_over_51/video/2018WCB_66_P2_SOLIS Osniel_CUB_ALSAEDI Mohammed_IRQ\n","73\n","==== running: yes | ffmpeg -i '/content/drive/.shortcut-targets-by-id/1wERFTJ3tk-moekRG-4KtAeT8uJewI1Y3/データ/3second_videos/game_over_51/2018WCB_66_P2_SOLIS Osniel_CUB_ALSAEDI Mohammed_IRQ_2.mp4' '/content/drive/MyDrive/Colab Notebooks/2023年度スポコン/データ/3second_videos/game_over_51/video/2018WCB_66_P2_SOLIS Osniel_CUB_ALSAEDI Mohammed_IRQ/object_removal/images/%04d.png'\n","Pretrained flow completion model has loaded...\n","Pretrained ProPainter has loaded...\n","Network [InpaintGenerator] was created. Total number of parameters: 39.4 million. To see the architecture, do print(network).\n","\n","Processing: images [73 frames]...\n","100% 15/15 [00:06<00:00,  2.45it/s]\n","IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (640, 360) to (640, 368) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n","IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (640, 360) to (640, 368) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n","\n","All results are saved in /content/drive/MyDrive/Colab Notebooks/2023年度スポコン/データ/3second_videos/game_over_51/video/2018WCB_66_P2_SOLIS Osniel_CUB_ALSAEDI Mohammed_IRQ/result/images\n","final text_encoder_type: bert-base-uncased\n","Model loaded from /root/.cache/huggingface/hub/models--ShilongLiu--GroundingDINO/snapshots/a94c9b567a2a374598f05c584e96798a170c56fb/groundingdino_swinb_cogcoor.pth \n"," => _IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n","/content/drive/MyDrive/Colab Notebooks/2023年度スポコン/データ/3second_videos/game_over_51/video/2018WCB_66_P2_SOLIS Osniel_CUB_ALSAEDI Mohammed_IRQ/result/images/inpaint_out.mp4\n","/content/drive/.shortcut-targets-by-id/1wERFTJ3tk-moekRG-4KtAeT8uJewI1Y3/データ/3second_videos/game_over_51/video/2018WCB_63_P2_HAECKER Katharina_AUS_VERMEER Sanne_NED\n","73\n","==== running: yes | ffmpeg -i '/content/drive/.shortcut-targets-by-id/1wERFTJ3tk-moekRG-4KtAeT8uJewI1Y3/データ/3second_videos/game_over_51/2018WCB_63_P2_HAECKER Katharina_AUS_VERMEER Sanne_NED_2.mp4' '/content/drive/MyDrive/Colab Notebooks/2023年度スポコン/データ/3second_videos/game_over_51/video/2018WCB_63_P2_HAECKER Katharina_AUS_VERMEER Sanne_NED/object_removal/images/%04d.png'\n","Pretrained flow completion model has loaded...\n","Pretrained ProPainter has loaded...\n","Network [InpaintGenerator] was created. Total number of parameters: 39.4 million. To see the architecture, do print(network).\n","\n","Processing: images [73 frames]...\n","100% 15/15 [00:06<00:00,  2.47it/s]\n","IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (640, 360) to (640, 368) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n","IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (640, 360) to (640, 368) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n","\n","All results are saved in /content/drive/MyDrive/Colab Notebooks/2023年度スポコン/データ/3second_videos/game_over_51/video/2018WCB_63_P2_HAECKER Katharina_AUS_VERMEER Sanne_NED/result/images\n","final text_encoder_type: bert-base-uncased\n","Model loaded from /root/.cache/huggingface/hub/models--ShilongLiu--GroundingDINO/snapshots/a94c9b567a2a374598f05c584e96798a170c56fb/groundingdino_swinb_cogcoor.pth \n"," => _IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n","/content/drive/MyDrive/Colab Notebooks/2023年度スポコン/データ/3second_videos/game_over_51/video/2018WCB_63_P2_HAECKER Katharina_AUS_VERMEER Sanne_NED/result/images/inpaint_out.mp4\n","/content/drive/.shortcut-targets-by-id/1wERFTJ3tk-moekRG-4KtAeT8uJewI1Y3/データ/3second_videos/game_over_51/video/2018WCB_60_P1_BASSOU Issam_MAR_ALIPIEV Valentin_BUL\n","73\n","==== running: yes | ffmpeg -i '/content/drive/.shortcut-targets-by-id/1wERFTJ3tk-moekRG-4KtAeT8uJewI1Y3/データ/3second_videos/game_over_51/2018WCB_60_P1_BASSOU Issam_MAR_ALIPIEV Valentin_BUL_2.mp4' '/content/drive/MyDrive/Colab Notebooks/2023年度スポコン/データ/3second_videos/game_over_51/video/2018WCB_60_P1_BASSOU Issam_MAR_ALIPIEV Valentin_BUL/object_removal/images/%04d.png'\n","Pretrained flow completion model has loaded...\n","Pretrained ProPainter has loaded...\n","Network [InpaintGenerator] was created. Total number of parameters: 39.4 million. To see the architecture, do print(network).\n","\n","Processing: images [73 frames]...\n","100% 15/15 [00:06<00:00,  2.44it/s]\n","IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (640, 360) to (640, 368) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n","IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (640, 360) to (640, 368) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n","\n","All results are saved in /content/drive/MyDrive/Colab Notebooks/2023年度スポコン/データ/3second_videos/game_over_51/video/2018WCB_60_P1_BASSOU Issam_MAR_ALIPIEV Valentin_BUL/result/images\n","final text_encoder_type: bert-base-uncased\n","Model loaded from /root/.cache/huggingface/hub/models--ShilongLiu--GroundingDINO/snapshots/a94c9b567a2a374598f05c584e96798a170c56fb/groundingdino_swinb_cogcoor.pth \n"," => _IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n","/content/drive/MyDrive/Colab Notebooks/2023年度スポコン/データ/3second_videos/game_over_51/video/2018WCB_60_P1_BASSOU Issam_MAR_ALIPIEV Valentin_BUL/result/images/inpaint_out.mp4\n","/content/drive/.shortcut-targets-by-id/1wERFTJ3tk-moekRG-4KtAeT8uJewI1Y3/データ/3second_videos/game_over_51/video/2018WCB_90_RF_NAGASAWA Kenta_JPN_USTOPIRIYON Komronshokh_TJK\n","73\n","==== running: yes | ffmpeg -i '/content/drive/.shortcut-targets-by-id/1wERFTJ3tk-moekRG-4KtAeT8uJewI1Y3/データ/3second_videos/game_over_51/2018WCB_90_RF_NAGASAWA Kenta_JPN_USTOPIRIYON Komronshokh_TJK_2.mp4' '/content/drive/MyDrive/Colab Notebooks/2023年度スポコン/データ/3second_videos/game_over_51/video/2018WCB_90_RF_NAGASAWA Kenta_JPN_USTOPIRIYON Komronshokh_TJK/object_removal/images/%04d.png'\n","Pretrained flow completion model has loaded...\n","Pretrained ProPainter has loaded...\n","Network [InpaintGenerator] was created. Total number of parameters: 39.4 million. To see the architecture, do print(network).\n","\n","Processing: images [73 frames]...\n","100% 15/15 [00:06<00:00,  2.50it/s]\n","IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (640, 360) to (640, 368) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n","IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (640, 360) to (640, 368) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n","\n","All results are saved in /content/drive/MyDrive/Colab Notebooks/2023年度スポコン/データ/3second_videos/game_over_51/video/2018WCB_90_RF_NAGASAWA Kenta_JPN_USTOPIRIYON Komronshokh_TJK/result/images\n","final text_encoder_type: bert-base-uncased\n","Model loaded from /root/.cache/huggingface/hub/models--ShilongLiu--GroundingDINO/snapshots/a94c9b567a2a374598f05c584e96798a170c56fb/groundingdino_swinb_cogcoor.pth \n"," => _IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n","/content/drive/MyDrive/Colab Notebooks/2023年度スポコン/データ/3second_videos/game_over_51/video/2018WCB_90_RF_NAGASAWA Kenta_JPN_USTOPIRIYON Komronshokh_TJK/result/images/inpaint_out.mp4\n","/content/drive/.shortcut-targets-by-id/1wERFTJ3tk-moekRG-4KtAeT8uJewI1Y3/データ/3second_videos/game_over_51/video/2018WCB_81_P3_DEL ORBE CORTORREAL Medickson_DOM_TONISTE Kristjan_EST\n","73\n","==== running: yes | ffmpeg -i '/content/drive/.shortcut-targets-by-id/1wERFTJ3tk-moekRG-4KtAeT8uJewI1Y3/データ/3second_videos/game_over_51/2018WCB_81_P3_DEL ORBE CORTORREAL Medickson_DOM_TONISTE Kristjan_EST_2.mp4' '/content/drive/MyDrive/Colab Notebooks/2023年度スポコン/データ/3second_videos/game_over_51/video/2018WCB_81_P3_DEL ORBE CORTORREAL Medickson_DOM_TONISTE Kristjan_EST/object_removal/images/%04d.png'\n","Pretrained flow completion model has loaded...\n","Pretrained ProPainter has loaded...\n","Network [InpaintGenerator] was created. Total number of parameters: 39.4 million. To see the architecture, do print(network).\n","\n","Processing: images [73 frames]...\n","100% 15/15 [00:06<00:00,  2.49it/s]\n","IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (640, 360) to (640, 368) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n","IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (640, 360) to (640, 368) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n","\n","All results are saved in /content/drive/MyDrive/Colab Notebooks/2023年度スポコン/データ/3second_videos/game_over_51/video/2018WCB_81_P3_DEL ORBE CORTORREAL Medickson_DOM_TONISTE Kristjan_EST/result/images\n","final text_encoder_type: bert-base-uncased\n","Model loaded from /root/.cache/huggingface/hub/models--ShilongLiu--GroundingDINO/snapshots/a94c9b567a2a374598f05c584e96798a170c56fb/groundingdino_swinb_cogcoor.pth \n"," => _IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n","/content/drive/MyDrive/Colab Notebooks/2023年度スポコン/データ/3second_videos/game_over_51/video/2018WCB_81_P3_DEL ORBE CORTORREAL Medickson_DOM_TONISTE Kristjan_EST/result/images/inpaint_out.mp4\n","/content/drive/.shortcut-targets-by-id/1wERFTJ3tk-moekRG-4KtAeT8uJewI1Y3/データ/3second_videos/game_over_51/video/2018WCB_73_PF_GANBAATAR Odbayar_MGL_HEYDAROV Hidayat_AZE\n","73\n","==== running: yes | ffmpeg -i '/content/drive/.shortcut-targets-by-id/1wERFTJ3tk-moekRG-4KtAeT8uJewI1Y3/データ/3second_videos/game_over_51/2018WCB_73_PF_GANBAATAR Odbayar_MGL_HEYDAROV Hidayat_AZE_2.mp4' '/content/drive/MyDrive/Colab Notebooks/2023年度スポコン/データ/3second_videos/game_over_51/video/2018WCB_73_PF_GANBAATAR Odbayar_MGL_HEYDAROV Hidayat_AZE/object_removal/images/%04d.png'\n","Pretrained flow completion model has loaded...\n","Pretrained ProPainter has loaded...\n","Network [InpaintGenerator] was created. Total number of parameters: 39.4 million. To see the architecture, do print(network).\n","\n","Processing: images [73 frames]...\n","100% 15/15 [00:05<00:00,  2.52it/s]\n","IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (640, 360) to (640, 368) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n","IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (640, 360) to (640, 368) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n","\n","All results are saved in /content/drive/MyDrive/Colab Notebooks/2023年度スポコン/データ/3second_videos/game_over_51/video/2018WCB_73_PF_GANBAATAR Odbayar_MGL_HEYDAROV Hidayat_AZE/result/images\n","final text_encoder_type: bert-base-uncased\n","Model loaded from /root/.cache/huggingface/hub/models--ShilongLiu--GroundingDINO/snapshots/a94c9b567a2a374598f05c584e96798a170c56fb/groundingdino_swinb_cogcoor.pth \n"," => _IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n","/content/drive/MyDrive/Colab Notebooks/2023年度スポコン/データ/3second_videos/game_over_51/video/2018WCB_73_PF_GANBAATAR Odbayar_MGL_HEYDAROV Hidayat_AZE/result/images/inpaint_out.mp4\n","/content/drive/.shortcut-targets-by-id/1wERFTJ3tk-moekRG-4KtAeT8uJewI1Y3/データ/3second_videos/game_over_51/video/2018WCB_63_P1_GWEND Edwige_ITA_BALDORJ Mungunchimeg_MGL\n","73\n","==== running: yes | ffmpeg -i '/content/drive/.shortcut-targets-by-id/1wERFTJ3tk-moekRG-4KtAeT8uJewI1Y3/データ/3second_videos/game_over_51/2018WCB_63_P1_GWEND Edwige_ITA_BALDORJ Mungunchimeg_MGL_2.mp4' '/content/drive/MyDrive/Colab Notebooks/2023年度スポコン/データ/3second_videos/game_over_51/video/2018WCB_63_P1_GWEND Edwige_ITA_BALDORJ Mungunchimeg_MGL/object_removal/images/%04d.png'\n","Pretrained flow completion model has loaded...\n","Pretrained ProPainter has loaded...\n","Network [InpaintGenerator] was created. Total number of parameters: 39.4 million. To see the architecture, do print(network).\n","\n","Processing: images [73 frames]...\n","100% 15/15 [00:06<00:00,  2.48it/s]\n","IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (640, 360) to (640, 368) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n","IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (640, 360) to (640, 368) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n","\n","All results are saved in /content/drive/MyDrive/Colab Notebooks/2023年度スポコン/データ/3second_videos/game_over_51/video/2018WCB_63_P1_GWEND Edwige_ITA_BALDORJ Mungunchimeg_MGL/result/images\n","final text_encoder_type: bert-base-uncased\n","Model loaded from /root/.cache/huggingface/hub/models--ShilongLiu--GroundingDINO/snapshots/a94c9b567a2a374598f05c584e96798a170c56fb/groundingdino_swinb_cogcoor.pth \n"," => _IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n","/content/drive/MyDrive/Colab Notebooks/2023年度スポコン/データ/3second_videos/game_over_51/video/2018WCB_63_P1_GWEND Edwige_ITA_BALDORJ Mungunchimeg_MGL/result/images/inpaint_out.mp4\n","/content/drive/.shortcut-targets-by-id/1wERFTJ3tk-moekRG-4KtAeT8uJewI1Y3/データ/3second_videos/game_over_51/video/2018WCB_60_P4_GARRIGOS Francisco_ESP_HUSEYNOV Karamat_AZE\n","73\n","==== running: yes | ffmpeg -i '/content/drive/.shortcut-targets-by-id/1wERFTJ3tk-moekRG-4KtAeT8uJewI1Y3/データ/3second_videos/game_over_51/2018WCB_60_P4_GARRIGOS Francisco_ESP_HUSEYNOV Karamat_AZE_2.mp4' '/content/drive/MyDrive/Colab Notebooks/2023年度スポコン/データ/3second_videos/game_over_51/video/2018WCB_60_P4_GARRIGOS Francisco_ESP_HUSEYNOV Karamat_AZE/object_removal/images/%04d.png'\n","Pretrained flow completion model has loaded...\n","Pretrained ProPainter has loaded...\n","Network [InpaintGenerator] was created. Total number of parameters: 39.4 million. To see the architecture, do print(network).\n","\n","Processing: images [73 frames]...\n","100% 15/15 [00:06<00:00,  2.46it/s]\n","IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (640, 360) to (640, 368) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n","IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (640, 360) to (640, 368) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n","\n","All results are saved in /content/drive/MyDrive/Colab Notebooks/2023年度スポコン/データ/3second_videos/game_over_51/video/2018WCB_60_P4_GARRIGOS Francisco_ESP_HUSEYNOV Karamat_AZE/result/images\n","final text_encoder_type: bert-base-uncased\n","Model loaded from /root/.cache/huggingface/hub/models--ShilongLiu--GroundingDINO/snapshots/a94c9b567a2a374598f05c584e96798a170c56fb/groundingdino_swinb_cogcoor.pth \n"," => _IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n","/content/drive/MyDrive/Colab Notebooks/2023年度スポコン/データ/3second_videos/game_over_51/video/2018WCB_60_P4_GARRIGOS Francisco_ESP_HUSEYNOV Karamat_AZE/result/images/inpaint_out.mp4\n","/content/drive/.shortcut-targets-by-id/1wERFTJ3tk-moekRG-4KtAeT8uJewI1Y3/データ/3second_videos/game_over_51/video/2019WCT_100_F_FONSECA Jorge_POR_ILYASOV Niyaz_RUS\n","73\n","==== running: yes | ffmpeg -i '/content/drive/.shortcut-targets-by-id/1wERFTJ3tk-moekRG-4KtAeT8uJewI1Y3/データ/3second_videos/game_over_51/2019WCT_100_F_FONSECA Jorge_POR_ILYASOV Niyaz_RUS_2.mp4' '/content/drive/MyDrive/Colab Notebooks/2023年度スポコン/データ/3second_videos/game_over_51/video/2019WCT_100_F_FONSECA Jorge_POR_ILYASOV Niyaz_RUS/object_removal/images/%04d.png'\n","Pretrained flow completion model has loaded...\n","Pretrained ProPainter has loaded...\n","Network [InpaintGenerator] was created. Total number of parameters: 39.4 million. To see the architecture, do print(network).\n","\n","Processing: images [73 frames]...\n","100% 15/15 [00:05<00:00,  2.50it/s]\n","IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (640, 360) to (640, 368) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n","IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (640, 360) to (640, 368) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n","\n","All results are saved in /content/drive/MyDrive/Colab Notebooks/2023年度スポコン/データ/3second_videos/game_over_51/video/2019WCT_100_F_FONSECA Jorge_POR_ILYASOV Niyaz_RUS/result/images\n","final text_encoder_type: bert-base-uncased\n","Model loaded from /root/.cache/huggingface/hub/models--ShilongLiu--GroundingDINO/snapshots/a94c9b567a2a374598f05c584e96798a170c56fb/groundingdino_swinb_cogcoor.pth \n"," => _IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n","/content/drive/MyDrive/Colab Notebooks/2023年度スポコン/データ/3second_videos/game_over_51/video/2019WCT_100_F_FONSECA Jorge_POR_ILYASOV Niyaz_RUS/result/images/inpaint_out.mp4\n","/content/drive/.shortcut-targets-by-id/1wERFTJ3tk-moekRG-4KtAeT8uJewI1Y3/データ/3second_videos/game_over_51/video/2019WCT_52_P2_EASTON Tinka_AUS_PERENC Agata_POL\n","73\n","==== running: yes | ffmpeg -i '/content/drive/.shortcut-targets-by-id/1wERFTJ3tk-moekRG-4KtAeT8uJewI1Y3/データ/3second_videos/game_over_51/2019WCT_52_P2_EASTON Tinka_AUS_PERENC Agata_POL_2.mp4' '/content/drive/MyDrive/Colab Notebooks/2023年度スポコン/データ/3second_videos/game_over_51/video/2019WCT_52_P2_EASTON Tinka_AUS_PERENC Agata_POL/object_removal/images/%04d.png'\n","Pretrained flow completion model has loaded...\n","Pretrained ProPainter has loaded...\n","Network [InpaintGenerator] was created. Total number of parameters: 39.4 million. To see the architecture, do print(network).\n","\n","Processing: images [73 frames]...\n","100% 15/15 [00:05<00:00,  2.52it/s]\n","IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (640, 360) to (640, 368) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n","IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (640, 360) to (640, 368) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n","\n","All results are saved in /content/drive/MyDrive/Colab Notebooks/2023年度スポコン/データ/3second_videos/game_over_51/video/2019WCT_52_P2_EASTON Tinka_AUS_PERENC Agata_POL/result/images\n","final text_encoder_type: bert-base-uncased\n","Model loaded from /root/.cache/huggingface/hub/models--ShilongLiu--GroundingDINO/snapshots/a94c9b567a2a374598f05c584e96798a170c56fb/groundingdino_swinb_cogcoor.pth \n"," => _IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n","/content/drive/MyDrive/Colab Notebooks/2023年度スポコン/データ/3second_videos/game_over_51/video/2019WCT_52_P2_EASTON Tinka_AUS_PERENC Agata_POL/result/images/inpaint_out.mp4\n","/content/drive/.shortcut-targets-by-id/1wERFTJ3tk-moekRG-4KtAeT8uJewI1Y3/データ/3second_videos/game_over_51/video/2021WCB_81_SF_CASSE Matthias_BEL_BOLTABOEV Sharofiddin_UZB\n","73\n","==== running: yes | ffmpeg -i '/content/drive/.shortcut-targets-by-id/1wERFTJ3tk-moekRG-4KtAeT8uJewI1Y3/データ/3second_videos/game_over_51/2021WCB_81_SF_CASSE Matthias_BEL_BOLTABOEV Sharofiddin_UZB_2.mp4' '/content/drive/MyDrive/Colab Notebooks/2023年度スポコン/データ/3second_videos/game_over_51/video/2021WCB_81_SF_CASSE Matthias_BEL_BOLTABOEV Sharofiddin_UZB/object_removal/images/%04d.png'\n","Pretrained flow completion model has loaded...\n","Pretrained ProPainter has loaded...\n","Network [InpaintGenerator] was created. Total number of parameters: 39.4 million. To see the architecture, do print(network).\n","\n","Processing: images [73 frames]...\n","100% 15/15 [00:06<00:00,  2.49it/s]\n","IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (640, 360) to (640, 368) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n","IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (640, 360) to (640, 368) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n","\n","All results are saved in /content/drive/MyDrive/Colab Notebooks/2023年度スポコン/データ/3second_videos/game_over_51/video/2021WCB_81_SF_CASSE Matthias_BEL_BOLTABOEV Sharofiddin_UZB/result/images\n","final text_encoder_type: bert-base-uncased\n","Model loaded from /root/.cache/huggingface/hub/models--ShilongLiu--GroundingDINO/snapshots/a94c9b567a2a374598f05c584e96798a170c56fb/groundingdino_swinb_cogcoor.pth \n"," => _IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n","/content/drive/MyDrive/Colab Notebooks/2023年度スポコン/データ/3second_videos/game_over_51/video/2021WCB_81_SF_CASSE Matthias_BEL_BOLTABOEV Sharofiddin_UZB/result/images/inpaint_out.mp4\n","/content/drive/.shortcut-targets-by-id/1wERFTJ3tk-moekRG-4KtAeT8uJewI1Y3/データ/3second_videos/game_over_51/video/2019WCT_73_P3_REITER Lukas_AUT_BASILE Fabio_ITA\n","73\n","==== running: yes | ffmpeg -i '/content/drive/.shortcut-targets-by-id/1wERFTJ3tk-moekRG-4KtAeT8uJewI1Y3/データ/3second_videos/game_over_51/2019WCT_73_P3_REITER Lukas_AUT_BASILE Fabio_ITA_2.mp4' '/content/drive/MyDrive/Colab Notebooks/2023年度スポコン/データ/3second_videos/game_over_51/video/2019WCT_73_P3_REITER Lukas_AUT_BASILE Fabio_ITA/object_removal/images/%04d.png'\n","Pretrained flow completion model has loaded...\n","Pretrained ProPainter has loaded...\n","Network [InpaintGenerator] was created. Total number of parameters: 39.4 million. To see the architecture, do print(network).\n","\n","Processing: images [73 frames]...\n","100% 15/15 [00:06<00:00,  2.47it/s]\n","IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (640, 360) to (640, 368) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n","IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (640, 360) to (640, 368) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n","\n","All results are saved in /content/drive/MyDrive/Colab Notebooks/2023年度スポコン/データ/3second_videos/game_over_51/video/2019WCT_73_P3_REITER Lukas_AUT_BASILE Fabio_ITA/result/images\n","final text_encoder_type: bert-base-uncased\n","Model loaded from /root/.cache/huggingface/hub/models--ShilongLiu--GroundingDINO/snapshots/a94c9b567a2a374598f05c584e96798a170c56fb/groundingdino_swinb_cogcoor.pth \n"," => _IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n","/content/drive/MyDrive/Colab Notebooks/2023年度スポコン/データ/3second_videos/game_over_51/video/2019WCT_73_P3_REITER Lukas_AUT_BASILE Fabio_ITA/result/images/inpaint_out.mp4\n","/content/drive/.shortcut-targets-by-id/1wERFTJ3tk-moekRG-4KtAeT8uJewI1Y3/データ/3second_videos/game_over_51/video/2019WCT_52_PF_ABE Uta_JPN_GILES Chelsie_GBR\n","73\n","==== running: yes | ffmpeg -i '/content/drive/.shortcut-targets-by-id/1wERFTJ3tk-moekRG-4KtAeT8uJewI1Y3/データ/3second_videos/game_over_51/2019WCT_52_PF_ABE Uta_JPN_GILES Chelsie_GBR_2.mp4' '/content/drive/MyDrive/Colab Notebooks/2023年度スポコン/データ/3second_videos/game_over_51/video/2019WCT_52_PF_ABE Uta_JPN_GILES Chelsie_GBR/object_removal/images/%04d.png'\n","Pretrained flow completion model has loaded...\n","Pretrained ProPainter has loaded...\n","Network [InpaintGenerator] was created. Total number of parameters: 39.4 million. To see the architecture, do print(network).\n","\n","Processing: images [73 frames]...\n","100% 15/15 [00:06<00:00,  2.48it/s]\n","IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (640, 360) to (640, 368) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n","IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (640, 360) to (640, 368) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n","\n","All results are saved in /content/drive/MyDrive/Colab Notebooks/2023年度スポコン/データ/3second_videos/game_over_51/video/2019WCT_52_PF_ABE Uta_JPN_GILES Chelsie_GBR/result/images\n","final text_encoder_type: bert-base-uncased\n","Model loaded from /root/.cache/huggingface/hub/models--ShilongLiu--GroundingDINO/snapshots/a94c9b567a2a374598f05c584e96798a170c56fb/groundingdino_swinb_cogcoor.pth \n"," => _IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n","/content/drive/MyDrive/Colab Notebooks/2023年度スポコン/データ/3second_videos/game_over_51/video/2019WCT_52_PF_ABE Uta_JPN_GILES Chelsie_GBR/result/images/inpaint_out.mp4\n","/content/drive/.shortcut-targets-by-id/1wERFTJ3tk-moekRG-4KtAeT8uJewI1Y3/データ/3second_videos/game_over_51/video/2019WCT_52_P1_LKHAGVASUREN Sosorbaram_MGL_STANGAR Anja_SLO\n","73\n","==== running: yes | ffmpeg -i '/content/drive/.shortcut-targets-by-id/1wERFTJ3tk-moekRG-4KtAeT8uJewI1Y3/データ/3second_videos/game_over_51/2019WCT_52_P1_LKHAGVASUREN Sosorbaram_MGL_STANGAR Anja_SLO_2.mp4' '/content/drive/MyDrive/Colab Notebooks/2023年度スポコン/データ/3second_videos/game_over_51/video/2019WCT_52_P1_LKHAGVASUREN Sosorbaram_MGL_STANGAR Anja_SLO/object_removal/images/%04d.png'\n","Pretrained flow completion model has loaded...\n","Pretrained ProPainter has loaded...\n","Network [InpaintGenerator] was created. Total number of parameters: 39.4 million. To see the architecture, do print(network).\n","\n","Processing: images [73 frames]...\n","100% 15/15 [00:05<00:00,  2.51it/s]\n","IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (640, 360) to (640, 368) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n","IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (640, 360) to (640, 368) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n","\n","All results are saved in /content/drive/MyDrive/Colab Notebooks/2023年度スポコン/データ/3second_videos/game_over_51/video/2019WCT_52_P1_LKHAGVASUREN Sosorbaram_MGL_STANGAR Anja_SLO/result/images\n","final text_encoder_type: bert-base-uncased\n","Model loaded from /root/.cache/huggingface/hub/models--ShilongLiu--GroundingDINO/snapshots/a94c9b567a2a374598f05c584e96798a170c56fb/groundingdino_swinb_cogcoor.pth \n"," => _IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n","/content/drive/MyDrive/Colab Notebooks/2023年度スポコン/データ/3second_videos/game_over_51/video/2019WCT_52_P1_LKHAGVASUREN Sosorbaram_MGL_STANGAR Anja_SLO/result/images/inpaint_out.mp4\n","/content/drive/.shortcut-targets-by-id/1wERFTJ3tk-moekRG-4KtAeT8uJewI1Y3/データ/3second_videos/game_over_51/video/2021WCB_73_P1_GJAKOVA Akil_KOS_MOHYELDIN Mohamed_EGY\n","73\n","==== running: yes | ffmpeg -i '/content/drive/.shortcut-targets-by-id/1wERFTJ3tk-moekRG-4KtAeT8uJewI1Y3/データ/3second_videos/game_over_51/2021WCB_73_P1_GJAKOVA Akil_KOS_MOHYELDIN Mohamed_EGY_2.mp4' '/content/drive/MyDrive/Colab Notebooks/2023年度スポコン/データ/3second_videos/game_over_51/video/2021WCB_73_P1_GJAKOVA Akil_KOS_MOHYELDIN Mohamed_EGY/object_removal/images/%04d.png'\n","Pretrained flow completion model has loaded...\n","Pretrained ProPainter has loaded...\n","Network [InpaintGenerator] was created. Total number of parameters: 39.4 million. To see the architecture, do print(network).\n","\n","Processing: images [73 frames]...\n","100% 15/15 [00:06<00:00,  2.46it/s]\n","IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (640, 360) to (640, 368) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n","IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (640, 360) to (640, 368) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n","\n","All results are saved in /content/drive/MyDrive/Colab Notebooks/2023年度スポコン/データ/3second_videos/game_over_51/video/2021WCB_73_P1_GJAKOVA Akil_KOS_MOHYELDIN Mohamed_EGY/result/images\n","final text_encoder_type: bert-base-uncased\n","Model loaded from /root/.cache/huggingface/hub/models--ShilongLiu--GroundingDINO/snapshots/a94c9b567a2a374598f05c584e96798a170c56fb/groundingdino_swinb_cogcoor.pth \n"," => _IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n","/content/drive/MyDrive/Colab Notebooks/2023年度スポコン/データ/3second_videos/game_over_51/video/2021WCB_73_P1_GJAKOVA Akil_KOS_MOHYELDIN Mohamed_EGY/result/images/inpaint_out.mp4\n","/content/drive/.shortcut-targets-by-id/1wERFTJ3tk-moekRG-4KtAeT8uJewI1Y3/データ/3second_videos/game_over_51/video/2019WCT_66_P3_SHMAILOV Baruch_ISR_TE Artur_KGZ\n","73\n","==== running: yes | ffmpeg -i '/content/drive/.shortcut-targets-by-id/1wERFTJ3tk-moekRG-4KtAeT8uJewI1Y3/データ/3second_videos/game_over_51/2019WCT_66_P3_SHMAILOV Baruch_ISR_TE Artur_KGZ_2.mp4' '/content/drive/MyDrive/Colab Notebooks/2023年度スポコン/データ/3second_videos/game_over_51/video/2019WCT_66_P3_SHMAILOV Baruch_ISR_TE Artur_KGZ/object_removal/images/%04d.png'\n","Pretrained flow completion model has loaded...\n","Pretrained ProPainter has loaded...\n","Network [InpaintGenerator] was created. Total number of parameters: 39.4 million. To see the architecture, do print(network).\n","\n","Processing: images [73 frames]...\n","100% 15/15 [00:05<00:00,  2.50it/s]\n","IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (640, 360) to (640, 368) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n","IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (640, 360) to (640, 368) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n","\n","All results are saved in /content/drive/MyDrive/Colab Notebooks/2023年度スポコン/データ/3second_videos/game_over_51/video/2019WCT_66_P3_SHMAILOV Baruch_ISR_TE Artur_KGZ/result/images\n","final text_encoder_type: bert-base-uncased\n","Model loaded from /root/.cache/huggingface/hub/models--ShilongLiu--GroundingDINO/snapshots/a94c9b567a2a374598f05c584e96798a170c56fb/groundingdino_swinb_cogcoor.pth \n"," => _IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n","/content/drive/MyDrive/Colab Notebooks/2023年度スポコン/データ/3second_videos/game_over_51/video/2019WCT_66_P3_SHMAILOV Baruch_ISR_TE Artur_KGZ/result/images/inpaint_out.mp4\n","/content/drive/.shortcut-targets-by-id/1wERFTJ3tk-moekRG-4KtAeT8uJewI1Y3/データ/3second_videos/game_over_51/video/2021WCB_60_P2_VERSTRAETEN Jorre_BEL_MOROCHO Steven_ECU\n","73\n","==== running: yes | ffmpeg -i '/content/drive/.shortcut-targets-by-id/1wERFTJ3tk-moekRG-4KtAeT8uJewI1Y3/データ/3second_videos/game_over_51/2021WCB_60_P2_VERSTRAETEN Jorre_BEL_MOROCHO Steven_ECU_2.mp4' '/content/drive/MyDrive/Colab Notebooks/2023年度スポコン/データ/3second_videos/game_over_51/video/2021WCB_60_P2_VERSTRAETEN Jorre_BEL_MOROCHO Steven_ECU/object_removal/images/%04d.png'\n","Pretrained flow completion model has loaded...\n","Pretrained ProPainter has loaded...\n","Network [InpaintGenerator] was created. Total number of parameters: 39.4 million. To see the architecture, do print(network).\n","\n","Processing: images [73 frames]...\n","100% 15/15 [00:05<00:00,  2.53it/s]\n","IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (640, 360) to (640, 368) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n","IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (640, 360) to (640, 368) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n","\n","All results are saved in /content/drive/MyDrive/Colab Notebooks/2023年度スポコン/データ/3second_videos/game_over_51/video/2021WCB_60_P2_VERSTRAETEN Jorre_BEL_MOROCHO Steven_ECU/result/images\n","final text_encoder_type: bert-base-uncased\n","Model loaded from /root/.cache/huggingface/hub/models--ShilongLiu--GroundingDINO/snapshots/a94c9b567a2a374598f05c584e96798a170c56fb/groundingdino_swinb_cogcoor.pth \n"," => _IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n","/content/drive/MyDrive/Colab Notebooks/2023年度スポコン/データ/3second_videos/game_over_51/video/2021WCB_60_P2_VERSTRAETEN Jorre_BEL_MOROCHO Steven_ECU/result/images/inpaint_out.mp4\n","/content/drive/.shortcut-targets-by-id/1wERFTJ3tk-moekRG-4KtAeT8uJewI1Y3/データ/3second_videos/game_over_51/video/2021WCB_66_P3_LOMBARDO Manuel_ITA_POLANCO Orlando_CUB\n","73\n","==== running: yes | ffmpeg -i '/content/drive/.shortcut-targets-by-id/1wERFTJ3tk-moekRG-4KtAeT8uJewI1Y3/データ/3second_videos/game_over_51/2021WCB_66_P3_LOMBARDO Manuel_ITA_POLANCO Orlando_CUB_2.mp4' '/content/drive/MyDrive/Colab Notebooks/2023年度スポコン/データ/3second_videos/game_over_51/video/2021WCB_66_P3_LOMBARDO Manuel_ITA_POLANCO Orlando_CUB/object_removal/images/%04d.png'\n","Pretrained flow completion model has loaded...\n","Pretrained ProPainter has loaded...\n","Network [InpaintGenerator] was created. Total number of parameters: 39.4 million. To see the architecture, do print(network).\n","\n","Processing: images [73 frames]...\n","100% 15/15 [00:06<00:00,  2.49it/s]\n","IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (640, 360) to (640, 368) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n","IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (640, 360) to (640, 368) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n","\n","All results are saved in /content/drive/MyDrive/Colab Notebooks/2023年度スポコン/データ/3second_videos/game_over_51/video/2021WCB_66_P3_LOMBARDO Manuel_ITA_POLANCO Orlando_CUB/result/images\n","final text_encoder_type: bert-base-uncased\n","Model loaded from /root/.cache/huggingface/hub/models--ShilongLiu--GroundingDINO/snapshots/a94c9b567a2a374598f05c584e96798a170c56fb/groundingdino_swinb_cogcoor.pth \n"," => _IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n","/content/drive/MyDrive/Colab Notebooks/2023年度スポコン/データ/3second_videos/game_over_51/video/2021WCB_66_P3_LOMBARDO Manuel_ITA_POLANCO Orlando_CUB/result/images/inpaint_out.mp4\n","/content/drive/.shortcut-targets-by-id/1wERFTJ3tk-moekRG-4KtAeT8uJewI1Y3/データ/3second_videos/game_over_51/video/2021WCB_60_PF_GARRIGOS Francisco_ESP_SHAMSHADIN Magzhan_KAZ\n","73\n","==== running: yes | ffmpeg -i '/content/drive/.shortcut-targets-by-id/1wERFTJ3tk-moekRG-4KtAeT8uJewI1Y3/データ/3second_videos/game_over_51/2021WCB_60_PF_GARRIGOS Francisco_ESP_SHAMSHADIN Magzhan_KAZ_2.mp4' '/content/drive/MyDrive/Colab Notebooks/2023年度スポコン/データ/3second_videos/game_over_51/video/2021WCB_60_PF_GARRIGOS Francisco_ESP_SHAMSHADIN Magzhan_KAZ/object_removal/images/%04d.png'\n","Pretrained flow completion model has loaded...\n","Pretrained ProPainter has loaded...\n","Network [InpaintGenerator] was created. Total number of parameters: 39.4 million. To see the architecture, do print(network).\n","\n","Processing: images [73 frames]...\n","100% 15/15 [00:06<00:00,  2.48it/s]\n","IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (640, 360) to (640, 368) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n","IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (640, 360) to (640, 368) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n","\n","All results are saved in /content/drive/MyDrive/Colab Notebooks/2023年度スポコン/データ/3second_videos/game_over_51/video/2021WCB_60_PF_GARRIGOS Francisco_ESP_SHAMSHADIN Magzhan_KAZ/result/images\n","final text_encoder_type: bert-base-uncased\n","Model loaded from /root/.cache/huggingface/hub/models--ShilongLiu--GroundingDINO/snapshots/a94c9b567a2a374598f05c584e96798a170c56fb/groundingdino_swinb_cogcoor.pth \n"," => _IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n","/content/drive/MyDrive/Colab Notebooks/2023年度スポコン/データ/3second_videos/game_over_51/video/2021WCB_60_PF_GARRIGOS Francisco_ESP_SHAMSHADIN Magzhan_KAZ/result/images/inpaint_out.mp4\n","/content/drive/.shortcut-targets-by-id/1wERFTJ3tk-moekRG-4KtAeT8uJewI1Y3/データ/3second_videos/game_over_51/video/2018WCB_70_PF_ALVEAR Yuri_COL_ARAI Chizuru_JPN\n","73\n","==== running: yes | ffmpeg -i '/content/drive/.shortcut-targets-by-id/1wERFTJ3tk-moekRG-4KtAeT8uJewI1Y3/データ/3second_videos/game_over_51/2018WCB_70_PF_ALVEAR Yuri_COL_ARAI Chizuru_JPN_2.mp4' '/content/drive/MyDrive/Colab Notebooks/2023年度スポコン/データ/3second_videos/game_over_51/video/2018WCB_70_PF_ALVEAR Yuri_COL_ARAI Chizuru_JPN/object_removal/images/%04d.png'\n","Pretrained flow completion model has loaded...\n","Pretrained ProPainter has loaded...\n","Network [InpaintGenerator] was created. Total number of parameters: 39.4 million. To see the architecture, do print(network).\n","\n","Processing: images [73 frames]...\n","100% 15/15 [00:06<00:00,  2.49it/s]\n","IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (640, 360) to (640, 368) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n","IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (640, 360) to (640, 368) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n","\n","All results are saved in /content/drive/MyDrive/Colab Notebooks/2023年度スポコン/データ/3second_videos/game_over_51/video/2018WCB_70_PF_ALVEAR Yuri_COL_ARAI Chizuru_JPN/result/images\n","final text_encoder_type: bert-base-uncased\n","Model loaded from /root/.cache/huggingface/hub/models--ShilongLiu--GroundingDINO/snapshots/a94c9b567a2a374598f05c584e96798a170c56fb/groundingdino_swinb_cogcoor.pth \n"," => _IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n","/content/drive/MyDrive/Colab Notebooks/2023年度スポコン/データ/3second_videos/game_over_51/video/2018WCB_70_PF_ALVEAR Yuri_COL_ARAI Chizuru_JPN/result/images/inpaint_out.mp4\n","/content/drive/.shortcut-targets-by-id/1wERFTJ3tk-moekRG-4KtAeT8uJewI1Y3/データ/3second_videos/game_over_51/video/2018WCB_60_P2_TRBOVC Matjaz_SLO_BESTAEV Otar_KGZ\n","73\n","==== running: yes | ffmpeg -i '/content/drive/.shortcut-targets-by-id/1wERFTJ3tk-moekRG-4KtAeT8uJewI1Y3/データ/3second_videos/game_over_51/2018WCB_60_P2_TRBOVC Matjaz_SLO_BESTAEV Otar_KGZ_2.mp4' '/content/drive/MyDrive/Colab Notebooks/2023年度スポコン/データ/3second_videos/game_over_51/video/2018WCB_60_P2_TRBOVC Matjaz_SLO_BESTAEV Otar_KGZ/object_removal/images/%04d.png'\n","FATAL: command failed\n"]},{"output_type":"error","ename":"SystemExit","evalue":"ignored","traceback":["An exception has occurred, use %tb to see the full traceback.\n","\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 256\n"]}],"source":["import warnings\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import requests\n","from PIL import Image\n","from io import BytesIO\n","from lang_sam import LangSAM\n","import os\n","import cv2\n","import torch\n","import gc\n","\n","str_dir = \"/content/drive/MyDrive/Colab Notebooks/2023年度スポコン/データ/3second_videos/game_over_51\"\n","\n","os.chdir(str_dir)\n","mv_ls = [os.getcwd() + \"/\" + file_name for file_name in os.listdir(os.getcwd()) if file_name[-6:-4] == \"_2\"]\n","\n","# text_prompt = \"people in white and blue on the yellow floor\"\n","# text_prompt = \"only the person in white and the person in blue on the yellow floor and the yellow floor.\"\n","# text_prompt = \"only two person in white and blue on the yellow floor and the yellow floor.\"\n","text_prompt = \"one person in the black suit on the yellow floor\"\n","# text_prompt = \"three person in white and in blue and in black on the yellow floor, the yellow floor.\"\n","\n","def download_image(url):\n","    response = requests.get(url)\n","    response.raise_for_status()\n","    return Image.open(BytesIO(response.content)).convert(\"RGB\")\n","\n","def save_mask(mask_np, filename):\n","    mask_image = Image.fromarray((mask_np * 255).astype(np.uint8))\n","    mask_image.save(filename)\n","\n","def display_image_with_masks(image, masks):\n","    num_masks = len(masks)\n","\n","    fig, axes = plt.subplots(1, num_masks + 1, figsize=(15, 5))\n","    axes[0].imshow(image)\n","    axes[0].set_title(\"Original Image\")\n","    axes[0].axis('off')\n","\n","    for i, mask_np in enumerate(masks):\n","        axes[i+1].imshow(mask_np, cmap='gray')\n","        axes[i+1].set_title(f\"Mask {i+1}\")\n","        axes[i+1].axis('off')\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","def display_image_with_boxes(image, boxes, logits):\n","    fig, ax = plt.subplots()\n","    ax.imshow(image)\n","    ax.set_title(\"Image with Bounding Boxes\")\n","    ax.axis('off')\n","\n","    for box, logit in zip(boxes, logits):\n","        x_min, y_min, x_max, y_max = box\n","        confidence_score = round(logit.item(), 2)  # Convert logit to a scalar before rounding\n","        box_width = x_max - x_min\n","        box_height = y_max - y_min\n","\n","        # Draw bounding box\n","        rect = plt.Rectangle((x_min, y_min), box_width, box_height, fill=False, edgecolor='red', linewidth=2)\n","        ax.add_patch(rect)\n","\n","        # Add confidence score as text\n","        ax.text(x_min, y_min, f\"Confidence: {confidence_score}\", fontsize=8, color='red', verticalalignment='top')\n","\n","    plt.show()\n","\n","def print_bounding_boxes(boxes):\n","    print(\"Bounding Boxes:\")\n","    for i, box in enumerate(boxes):\n","        print(f\"Box {i+1}: {box}\")\n","\n","def print_detected_phrases(phrases):\n","    print(\"\\nDetected Phrases:\")\n","    for i, phrase in enumerate(phrases):\n","        print(f\"Phrase {i+1}: {phrase}\")\n","\n","def print_logits(logits):\n","    print(\"\\nConfidence:\")\n","    for i, logit in enumerate(logits):\n","        print(f\"Logit {i+1}: {logit}\")\n","\n","def do_system(arg):\n","\tprint(f\"==== running: {arg}\")\n","\terr = os.system(arg)\n","\tif err:\n","\t\tprint(\"FATAL: command failed\")\n","\t\tsys.exit(err)\n","\n","def main():\n","    # Suppress warning messages\n","    warnings.filterwarnings(\"ignore\")\n","    text_prompt = \"one person in the black suit on the yellow floor\"\n","    try:\n","        # if image.startswith(\"http\"):\n","        #     image_pil = download_image(image)\n","        # else:\n","        #     image_pil = Image.open(image).convert(\"RGB\")\n","\n","        model = LangSAM()\n","        img_path_ls = []\n","        fps_ls = []\n","        start_frame = 0\n","\n","        for video_name, video_path in enumerate(mv_ls):\n","          count = 0\n","\n","          cap = cv2.VideoCapture(video_path)\n","          cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n","          os.chdir(str_dir)\n","\n","          fps_ls.append(int(cap.get(cv2.CAP_PROP_FPS)))\n","\n","          img_path = os.path.join(os.getcwd() ,\"video\" ,[file_name for file_name in os.listdir(os.getcwd()) if file_name[-6:-4] == \"_2\"][video_name][:-6])\n","          print(img_path)\n","          img_path_ls.append(img_path)\n","          # 出力ディレクトリが存在しない場合、作成します。\n","          if not os.path.exists(img_path):\n","              os.makedirs(img_path)\n","\n","          while cap.isOpened():\n","            torch.cuda.empty_cache()\n","            # del variables\n","            gc.collect()\n","\n","            torch.cuda.memory_summary(device=None, abbreviated=False)\n","\n","            ret, frame = cap.read()\n","\n","            if not ret:\n","              # print(\"no image\")\n","              break\n","\n","            image_pil = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n","\n","            masks, boxes, phrases, logits = model.predict(image_pil, text_prompt)\n","\n","            if len(masks) == 0:\n","              print(f\"No objects of the '{text_prompt}' prompt detected in the image.\")\n","            else:\n","                try:\n","                  ## Convert masks to numpy arrays\n","                  masks_np = [mask.squeeze().cpu().numpy() for mask in masks]\n","\n","                  ## 審判以外の予測を取り出す\n","                  player_label_idx = []\n","                  for idx, phrase in enumerate(phrases):\n","                    if not ('the yellow floor the yellow floor' in phrase or \"the yellow floor\" in phrase):\n","\n","                      player_label_idx.append(idx)\n","\n","                  # print(player_label_idx)\n","                  masks_np = [masks_np[idx] for idx in player_label_idx]\n","\n","                  ## 全てのマスクを合わせる\n","                  masks_np = sum(masks_np).astype(bool)\n","\n","                  # masks_np = cv2.cvtColor(masks_np.astype(np.uint8) * 255, cv2.COLOR_GRAY2BGR)\n","                  masks_np = cv2.cvtColor(masks_np.astype(np.uint8), cv2.COLOR_GRAY2BGR)\n","                  # masks_np = np.concatenate([masks_np, masks_np, masks_np])\n","                  frame = cv2.cvtColor(np.array(image_pil), cv2.COLOR_RGB2BGR)\n","\n","                  #result_img = cv2.bitwise_and(frame, masks_np)  #切り向き画像\n","                  masks_np = cv2.cvtColor(masks_np, cv2.COLOR_BGR2GRAY)\n","\n","                  img_path, masks_path, output_path = xmem(video_path, masks_np, video_name, str_dir)\n","\n","                  # The first example (object removal)\n","                  os.chdir(\"/content/ProPainter\")\n","\n","                  !python inference_propainter.py --video \"{img_path}\" --mask \"{masks_path}\" --output \"{output_path}\"\n","\n","                  #\n","\n","                  inpaint_path = os.path.join(output_path ,\"images\" ,\"inpaint_out.mp4\")\n","                  all_video_lang_segment_anything(inpaint_path, output_path)\n","                  break\n","                except Exception as e:\n","                  print(e, video_path)\n","                  # fps_ls[-1] = 0\n","                  continue\n","\n","\n","    except (requests.exceptions.RequestException, IOError) as e:\n","      print(f\"Error: {e}\")\n","\n","    return img_path_ls, fps_ls\n","\n","if __name__ == \"__main__\":\n","  # main()\n","  img_path_ls, fps_ls = main()\n","  for i, fps in zip(img_path_ls, fps_ls):\n","    # if fps !=0:\n","    #   i = os.path.join(i, \"images\")\n","    #   video_name = i.split(\"/\")[-2]\n","    #   do_system(f\"yes | ffmpeg -framerate {str(fps)} -i '{i}/%04d.png' -pix_fmt yuv420p '{i}/{video_name}.mp4'\")\n","      i = os.path.join(i, \"images\")\n","      video_name = i.split(\"/\")[-2]\n","      do_system(f\"yes | ffmpeg -framerate {str(fps)} -i '{i}/%04d.png' -pix_fmt yuv420p '{i}/{video_name}.mp4'\")"]},{"cell_type":"markdown","metadata":{"id":"pCRP1KLJjJv1"},"source":["## 2019"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5IEwiUc_5bUr"},"outputs":[],"source":["# !cp -r \"/content/drive/MyDrive/Colab Notebooks/2023年度スポコン/データ/2019WC_3seconds\" \"/content/drive/MyDrive/Colab Notebooks/2023年度スポコン\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IrBIXN3v5bUr"},"outputs":[],"source":["import warnings\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import requests\n","from PIL import Image\n","from io import BytesIO\n","from lang_sam import LangSAM\n","import os\n","import cv2\n","import torch\n","import gc\n","\n","str_dir = \"/content/drive/MyDrive/Colab Notebooks/2023年度スポコン/データ/2019WC_3seconds\"\n","\n","os.chdir(str_dir)\n","mv_ls = [os.getcwd() + \"/\" + file_name for file_name in os.listdir(os.getcwd()) if file_name[-6:-4] == \"_1\"]\n","\n","# text_prompt = \"people in white and blue on the yellow floor\"\n","# text_prompt = \"only the person in white and the person in blue on the yellow floor and the yellow floor.\"\n","# text_prompt = \"only two person in white and blue on the yellow floor and the yellow floor.\"\n","text_prompt = \"one person in the black suit on the yellow floor\"\n","# text_prompt = \"three person in white and in blue and in black on the yellow floor, the yellow floor.\"\n","\n","def download_image(url):\n","    response = requests.get(url)\n","    response.raise_for_status()\n","    return Image.open(BytesIO(response.content)).convert(\"RGB\")\n","\n","def save_mask(mask_np, filename):\n","    mask_image = Image.fromarray((mask_np * 255).astype(np.uint8))\n","    mask_image.save(filename)\n","\n","def display_image_with_masks(image, masks):\n","    num_masks = len(masks)\n","\n","    fig, axes = plt.subplots(1, num_masks + 1, figsize=(15, 5))\n","    axes[0].imshow(image)\n","    axes[0].set_title(\"Original Image\")\n","    axes[0].axis('off')\n","\n","    for i, mask_np in enumerate(masks):\n","        axes[i+1].imshow(mask_np, cmap='gray')\n","        axes[i+1].set_title(f\"Mask {i+1}\")\n","        axes[i+1].axis('off')\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","def display_image_with_boxes(image, boxes, logits):\n","    fig, ax = plt.subplots()\n","    ax.imshow(image)\n","    ax.set_title(\"Image with Bounding Boxes\")\n","    ax.axis('off')\n","\n","    for box, logit in zip(boxes, logits):\n","        x_min, y_min, x_max, y_max = box\n","        confidence_score = round(logit.item(), 2)  # Convert logit to a scalar before rounding\n","        box_width = x_max - x_min\n","        box_height = y_max - y_min\n","\n","        # Draw bounding box\n","        rect = plt.Rectangle((x_min, y_min), box_width, box_height, fill=False, edgecolor='red', linewidth=2)\n","        ax.add_patch(rect)\n","\n","        # Add confidence score as text\n","        ax.text(x_min, y_min, f\"Confidence: {confidence_score}\", fontsize=8, color='red', verticalalignment='top')\n","\n","    plt.show()\n","\n","def print_bounding_boxes(boxes):\n","    print(\"Bounding Boxes:\")\n","    for i, box in enumerate(boxes):\n","        print(f\"Box {i+1}: {box}\")\n","\n","def print_detected_phrases(phrases):\n","    print(\"\\nDetected Phrases:\")\n","    for i, phrase in enumerate(phrases):\n","        print(f\"Phrase {i+1}: {phrase}\")\n","\n","def print_logits(logits):\n","    print(\"\\nConfidence:\")\n","    for i, logit in enumerate(logits):\n","        print(f\"Logit {i+1}: {logit}\")\n","\n","def do_system(arg):\n","\tprint(f\"==== running: {arg}\")\n","\terr = os.system(arg)\n","\tif err:\n","\t\tprint(\"FATAL: command failed\")\n","\t\tsys.exit(err)\n","\n","def main():\n","    # Suppress warning messages\n","    warnings.filterwarnings(\"ignore\")\n","    text_prompt = \"one person in the black suit on the yellow floor\"\n","    try:\n","        # if image.startswith(\"http\"):\n","        #     image_pil = download_image(image)\n","        # else:\n","        #     image_pil = Image.open(image).convert(\"RGB\")\n","\n","        model = LangSAM()\n","        img_path_ls = []\n","        fps_ls = []\n","        start_frame = 0\n","\n","        for video_name, video_path in enumerate(mv_ls):\n","          count = 0\n","\n","          cap = cv2.VideoCapture(video_path)\n","          cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n","          os.chdir(str_dir)\n","\n","          fps_ls.append(int(cap.get(cv2.CAP_PROP_FPS)))\n","\n","          img_path = os.path.join(os.getcwd() ,\"video\" ,[file_name for file_name in os.listdir(os.getcwd()) if file_name[-6:-4] == \"_1\"][video_name][:-6])\n","          print(img_path)\n","          img_path_ls.append(img_path)\n","          # 出力ディレクトリが存在しない場合、作成します。\n","          if not os.path.exists(img_path):\n","              os.makedirs(img_path)\n","\n","          while cap.isOpened():\n","            torch.cuda.empty_cache()\n","            # del variables\n","            gc.collect()\n","\n","            torch.cuda.memory_summary(device=None, abbreviated=False)\n","\n","            ret, frame = cap.read()\n","\n","            if not ret:\n","              # print(\"no image\")\n","              break\n","\n","            image_pil = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n","\n","            masks, boxes, phrases, logits = model.predict(image_pil, text_prompt)\n","\n","            if len(masks) == 0:\n","              print(f\"No objects of the '{text_prompt}' prompt detected in the image.\")\n","            else:\n","                try:\n","                  ## Convert masks to numpy arrays\n","                  masks_np = [mask.squeeze().cpu().numpy() for mask in masks]\n","\n","                  ## 審判以外の予測を取り出す\n","                  player_label_idx = []\n","                  for idx, phrase in enumerate(phrases):\n","                    if not ('the yellow floor the yellow floor' in phrase or \"the yellow floor\" in phrase):\n","\n","                      player_label_idx.append(idx)\n","\n","                  # print(player_label_idx)\n","                  masks_np = [masks_np[idx] for idx in player_label_idx]\n","\n","                  ## 全てのマスクを合わせる\n","                  masks_np = sum(masks_np).astype(bool)\n","\n","                  # masks_np = cv2.cvtColor(masks_np.astype(np.uint8) * 255, cv2.COLOR_GRAY2BGR)\n","                  masks_np = cv2.cvtColor(masks_np.astype(np.uint8), cv2.COLOR_GRAY2BGR)\n","                  # masks_np = np.concatenate([masks_np, masks_np, masks_np])\n","                  frame = cv2.cvtColor(np.array(image_pil), cv2.COLOR_RGB2BGR)\n","\n","                  #result_img = cv2.bitwise_and(frame, masks_np)  #切り向き画像\n","                  masks_np = cv2.cvtColor(masks_np, cv2.COLOR_BGR2GRAY)\n","\n","                  img_path, masks_path, output_path = xmem(video_path, masks_np, video_name, str_dir)\n","\n","                  # The first example (object removal)\n","                  os.chdir(\"/content/ProPainter\")\n","\n","                  !python inference_propainter.py --video \"{img_path}\" --mask \"{masks_path}\" --output \"{output_path}\"\n","\n","                  #\n","\n","                  inpaint_path = os.path.join(output_path ,\"images\" ,\"inpaint_out.mp4\")\n","                  all_video_lang_segment_anything(inpaint_path, output_path)\n","                  break\n","                except Exception as e:\n","                  print(e, video_path)\n","                  # fps_ls[-1] = 0\n","                  continue\n","\n","\n","    except (requests.exceptions.RequestException, IOError) as e:\n","      print(f\"Error: {e}\")\n","\n","    return img_path_ls, fps_ls\n","\n","if __name__ == \"__main__\":\n","  # main()\n","  img_path_ls, fps_ls = main()\n","  for i, fps in zip(img_path_ls, fps_ls):\n","    # if fps !=0:\n","    #   i = os.path.join(i, \"images\")\n","    #   video_name = i.split(\"/\")[-2]\n","    #   do_system(f\"yes | ffmpeg -framerate {str(fps)} -i '{i}/%04d.png' -pix_fmt yuv420p '{i}/{video_name}.mp4'\")\n","      i = os.path.join(i, \"images\")\n","      video_name = i.split(\"/\")[-2]\n","      do_system(f\"yes | ffmpeg -framerate {str(fps)} -i '{i}/%04d.png' -pix_fmt yuv420p '{i}/{video_name}.mp4'\")"]},{"cell_type":"code","source":["os.makedirs('/content/drive/MyDrive/Colab Notebooks/2023年度スポコン/データ/3second_videos/game_over', exist_ok=True)"],"metadata":{"id":"K2wGDH0aODoe","executionInfo":{"status":"ok","timestamp":1701861642686,"user_tz":-540,"elapsed":435,"user":{"displayName":"盛拓矢","userId":"00009370238873879878"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["import os\n","import shutil\n","root_dir = '/content/drive/MyDrive/Colab Notebooks/2023年度スポコン/データ/3second_videos/game_over_51/video'\n","video_names = os.listdir(root_dir)\n","for idx, video_name in enumerate(video_names):\n","  if video_name != '2021WCB_P78_P2_SOUZA Beatriz_BRA_FONTAINE Lea_FRA':\n","    # print(idx, video_name)\n","    video_img_path = os.path.join(root_dir, video_name, 'images')\n","    # img_ls = sorted([imgs for imgs in os.listdir(video_img_path) if imgs[-3:]=='png'])\n","    os.system(f\"yes | ffmpeg -r 20 -i '{video_img_path}/%04d.png' -pix_fmt yuv420p '/content/drive/MyDrive/Colab Notebooks/2023年度スポコン/データ/3second_videos/game_over/{video_name}_2.mp4'\")"],"metadata":{"id":"G5Hf711JNog1","executionInfo":{"status":"ok","timestamp":1701861871705,"user_tz":-540,"elapsed":121013,"user":{"displayName":"盛拓矢","userId":"00009370238873879878"}}},"execution_count":4,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[],"gpuType":"A100"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"4a4c332e0f4c45ad9db6809706776ad9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_324009fa4a984f93889426b4056a8371","IPY_MODEL_6da26bf6e6504a25afae58d80485470c","IPY_MODEL_4c05b7f8ea144f50b656cf99380779bd"],"layout":"IPY_MODEL_21a93515b46d4c40b8298a3e388eeab8"}},"324009fa4a984f93889426b4056a8371":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1288bc8a1dff4665bc9dc588c3f3ef71","placeholder":"​","style":"IPY_MODEL_d3b02a9a644f4320967adf8764902db8","value":"Downloading (…)ingDINO_SwinB.cfg.py: 100%"}},"6da26bf6e6504a25afae58d80485470c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0a52ccc67e7c4f1486283482afd03388","max":1007,"min":0,"orientation":"horizontal","style":"IPY_MODEL_afb2f442cb7d499094de6727cb421748","value":1007}},"4c05b7f8ea144f50b656cf99380779bd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_043df64206a24d07b404c75f2bf04030","placeholder":"​","style":"IPY_MODEL_88a9a408c3974df58e85f98f6d373e83","value":" 1.01k/1.01k [00:00&lt;00:00, 82.6kB/s]"}},"21a93515b46d4c40b8298a3e388eeab8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1288bc8a1dff4665bc9dc588c3f3ef71":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d3b02a9a644f4320967adf8764902db8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0a52ccc67e7c4f1486283482afd03388":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"afb2f442cb7d499094de6727cb421748":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"043df64206a24d07b404c75f2bf04030":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"88a9a408c3974df58e85f98f6d373e83":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"facccd8f261b428b968f843450a50103":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ba6a5383e9b34036bd18e0e874db0fe3","IPY_MODEL_cb2368e4d433496893af80c0bba7d0b5","IPY_MODEL_f0ea551633d74aec94e219a60596b7e6"],"layout":"IPY_MODEL_46cd43c8662146bda602965accf55f0e"}},"ba6a5383e9b34036bd18e0e874db0fe3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_51493f9945f54831947c61dd86abf5b6","placeholder":"​","style":"IPY_MODEL_f13e4f66f2d04c7c93b30a1d01145de6","value":"Downloading tokenizer_config.json: 100%"}},"cb2368e4d433496893af80c0bba7d0b5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b0c49c67e1344c3c8ab20006240ee840","max":28,"min":0,"orientation":"horizontal","style":"IPY_MODEL_046aaf68d02f4e84b1aec66fb4f930a4","value":28}},"f0ea551633d74aec94e219a60596b7e6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bcd0156a0c744a54b1e15b9e387c59f8","placeholder":"​","style":"IPY_MODEL_67b61288f1b24734997a600ca0b99093","value":" 28.0/28.0 [00:00&lt;00:00, 2.11kB/s]"}},"46cd43c8662146bda602965accf55f0e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"51493f9945f54831947c61dd86abf5b6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f13e4f66f2d04c7c93b30a1d01145de6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b0c49c67e1344c3c8ab20006240ee840":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"046aaf68d02f4e84b1aec66fb4f930a4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"bcd0156a0c744a54b1e15b9e387c59f8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"67b61288f1b24734997a600ca0b99093":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4e9cf9f3290841748ddbd4613340e2bf":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f4840e6c5df74b049fab512e0682debf","IPY_MODEL_025f064f2d4c4bce9d465fa605106bb0","IPY_MODEL_4648860d9cc14e12855f92ef0e8220a0"],"layout":"IPY_MODEL_0e44ed3b6ba64025bc757d0bba61ed30"}},"f4840e6c5df74b049fab512e0682debf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7e01add16d3f4172a7ee5bfd2b555a65","placeholder":"​","style":"IPY_MODEL_28ad258b155a4358922ef86f7f1ac4be","value":"Downloading config.json: 100%"}},"025f064f2d4c4bce9d465fa605106bb0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_98bf6bfa9307414f9fa7e2f947298660","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f06f1e386e4b4044b0a549efb356a25d","value":570}},"4648860d9cc14e12855f92ef0e8220a0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_034cd10ad6ee413fa80cc5b7d87f43d9","placeholder":"​","style":"IPY_MODEL_4739f58f4c3940cd95ea1ef8406cbbb0","value":" 570/570 [00:00&lt;00:00, 48.3kB/s]"}},"0e44ed3b6ba64025bc757d0bba61ed30":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7e01add16d3f4172a7ee5bfd2b555a65":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"28ad258b155a4358922ef86f7f1ac4be":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"98bf6bfa9307414f9fa7e2f947298660":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f06f1e386e4b4044b0a549efb356a25d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"034cd10ad6ee413fa80cc5b7d87f43d9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4739f58f4c3940cd95ea1ef8406cbbb0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"34c33ee5d04141d4ac0101aae533e3ba":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_dbb068862ec14008899b5e45046c21fe","IPY_MODEL_e11120c204a24f3c9730e25873b768df","IPY_MODEL_36ae4b62e90947f8be33a2664330951e"],"layout":"IPY_MODEL_e9b7725bbadc43fa80ffab91834c4296"}},"dbb068862ec14008899b5e45046c21fe":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dd3d8cdab42c4e0eb2a79aaa58d927b4","placeholder":"​","style":"IPY_MODEL_50ad5f2b5a224763adcbc0ba589fd9fe","value":"Downloading vocab.txt: 100%"}},"e11120c204a24f3c9730e25873b768df":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_33e8628751b9418ab2812e873e21970c","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a7d0cde641d84ecea043b17ba1322efb","value":231508}},"36ae4b62e90947f8be33a2664330951e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_78ea65ecf7454276be9daa72873dfc65","placeholder":"​","style":"IPY_MODEL_da821e5bfdec41b78ca6fe6d3575f84f","value":" 232k/232k [00:00&lt;00:00, 5.07MB/s]"}},"e9b7725bbadc43fa80ffab91834c4296":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dd3d8cdab42c4e0eb2a79aaa58d927b4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"50ad5f2b5a224763adcbc0ba589fd9fe":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"33e8628751b9418ab2812e873e21970c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a7d0cde641d84ecea043b17ba1322efb":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"78ea65ecf7454276be9daa72873dfc65":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"da821e5bfdec41b78ca6fe6d3575f84f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2c69ab15a33c431d914ccb48fcc6cef0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e21930f801764a019e9045e7d97b89d6","IPY_MODEL_3e34e9bc477a469992bbb2d6f1428768","IPY_MODEL_a347962c3aff474ea254340f671e4c3e"],"layout":"IPY_MODEL_59ec03fe560c4f458f2c462371933978"}},"e21930f801764a019e9045e7d97b89d6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_61627101af9144f5a86c06eec174835a","placeholder":"​","style":"IPY_MODEL_dfcf23be7b5c4566abffc2140b991bde","value":"Downloading tokenizer.json: 100%"}},"3e34e9bc477a469992bbb2d6f1428768":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e4794e14a73f47faa155a2cd46340680","max":466062,"min":0,"orientation":"horizontal","style":"IPY_MODEL_384f43c372f2441f9b5cfe7024b782a3","value":466062}},"a347962c3aff474ea254340f671e4c3e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7f240899b56d4119a315fb2d83eb409a","placeholder":"​","style":"IPY_MODEL_58dae029e71446c480d96ceccf1bb8e4","value":" 466k/466k [00:00&lt;00:00, 25.2MB/s]"}},"59ec03fe560c4f458f2c462371933978":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"61627101af9144f5a86c06eec174835a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dfcf23be7b5c4566abffc2140b991bde":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e4794e14a73f47faa155a2cd46340680":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"384f43c372f2441f9b5cfe7024b782a3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7f240899b56d4119a315fb2d83eb409a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"58dae029e71446c480d96ceccf1bb8e4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f7a0dbc2911c442ba8b7d32832803189":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ec7418d2c8ac4467b7e567611044d816","IPY_MODEL_5b978e529f784680b9294593a90433a0","IPY_MODEL_8db1f8e1951842b3b7df923771f1e46a"],"layout":"IPY_MODEL_437aa8d6f64340d49168e6771ed2faab"}},"ec7418d2c8ac4467b7e567611044d816":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ec87e04430484432810977f16acb3f5e","placeholder":"​","style":"IPY_MODEL_8d050e19e5b1492cbe6c22a03582b0bd","value":"Downloading model.safetensors: 100%"}},"5b978e529f784680b9294593a90433a0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ee62f1467ed54a838e7f5513e0ac692d","max":440449768,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e59cc7c51db14c0088bb81c9c97c9f7f","value":440449768}},"8db1f8e1951842b3b7df923771f1e46a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_019cb61c5b464b83bb4e4b02db51f0b9","placeholder":"​","style":"IPY_MODEL_13855d6389844d93a91830cc8efbdbe0","value":" 440M/440M [00:02&lt;00:00, 212MB/s]"}},"437aa8d6f64340d49168e6771ed2faab":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ec87e04430484432810977f16acb3f5e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8d050e19e5b1492cbe6c22a03582b0bd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ee62f1467ed54a838e7f5513e0ac692d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e59cc7c51db14c0088bb81c9c97c9f7f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"019cb61c5b464b83bb4e4b02db51f0b9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"13855d6389844d93a91830cc8efbdbe0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"970e397cf8e2473a96b5308100eaf9cd":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_28946126c46141bbb15f7c3939d1f334","IPY_MODEL_90427614a762429fb1b169a307d2617b","IPY_MODEL_99bbf751a2524812ae502cb3fe20ccab"],"layout":"IPY_MODEL_bf5cf79e157b47f4a63b924dd6e3e061"}},"28946126c46141bbb15f7c3939d1f334":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_608397eb6fbd4eb8ba31cf96400479d6","placeholder":"​","style":"IPY_MODEL_7537404796be490fbcfb560973d80ee7","value":"Downloading (…)no_swinb_cogcoor.pth: 100%"}},"90427614a762429fb1b169a307d2617b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1297f14e5331414d92d5a6d1b6dc67bb","max":938057991,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6be379671c7f406e9b754c208ca1b2c5","value":938057991}},"99bbf751a2524812ae502cb3fe20ccab":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_514672913ef048fcb38f65f7bccc083b","placeholder":"​","style":"IPY_MODEL_9aef13e9c47a424c8bd35b6cf38b1710","value":" 938M/938M [00:04&lt;00:00, 200MB/s]"}},"bf5cf79e157b47f4a63b924dd6e3e061":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"608397eb6fbd4eb8ba31cf96400479d6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7537404796be490fbcfb560973d80ee7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1297f14e5331414d92d5a6d1b6dc67bb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6be379671c7f406e9b754c208ca1b2c5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"514672913ef048fcb38f65f7bccc083b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9aef13e9c47a424c8bd35b6cf38b1710":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}